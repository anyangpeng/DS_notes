{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gradient Descent**\n",
    "\n",
    "## **Concept**\n",
    "A first-order iterative optimization algorithms for finding a local minimum of a differenetiable function $\\Longrightarrow$ cost function\n",
    "* measures the local gradient of the error function with regards to the parameter vector $\\theta$, and it goes in the direction of descending gradient.\n",
    "* **Steps**\n",
    "    * 1. random initialization, filling $\\theta$ with random values;\n",
    "    * 2. improve it gradually, descrease the cost function (e.g., MSE);\n",
    "    * 3. converge to a minmum.\\\n",
    "![](images/1_1_gradient_descent.png)\n",
    "    \n",
    "#### **Learning hyperparameter**\n",
    "* learning rate too small $\\Longrightarrow$ long time to converge\n",
    "* learning rate too large $\\Longrightarrow$ might jump across the valley\n",
    "* **Local minimum** vs **Global minimum**\n",
    "    \n",
    "#### **MSE cost function** \n",
    "* **not** all regular bowl shape: having holes, ridges, plateaus, and/or sorts of irregular terrains\n",
    "    * difficult to converge to the minimum\\\n",
    "![](images/1_1_cost_function.png)\n",
    "\n",
    "* **Linear Regression model**: convex function\n",
    "    * line jointing two points never crosses the curve $\\Longrightarrow$ **only** global minimum (wait long enough &  training rate not too high)\n",
    "    * a continuous function that slop never change abruptly\n",
    "    * **Warning**: Using Gradient Descent $\\Longrightarrow$ **Scaling all variables first**\n",
    "    \n",
    "#### **Batch Gradient Descent**\n",
    "* Partial derivatives of the cost function:\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} = \\frac2{m} \\sum_{i=0}^m (\\theta^ T\\cdot{x^i} - y^i)x_j^i$$  \n",
    "* Gradient vector of the cost function:\n",
    "$$\\bigtriangledown_\\theta MSE(\\theta) = \\frac2{m}X^T\\cdot{(X\\cdot{\\theta} - y)}$$\n",
    "This uses the whole batch of training data at every step ---- > slow\n",
    "* scaling well with numerous features\n",
    "    * linear regression with hundreds of thundsands of features $\\Longrightarrow$ much faster using Gradient Descent\n",
    "    \n",
    "#### **Gradient Descent Step**\n",
    "$$\\theta^{(next \\ step)} = \\theta - \\eta\\bigtriangledown_\\theta MSE(\\theta)$$\n",
    "    \n",
    "* learning rate (eta)\n",
    "* **Grid Search** $\\Longrightarrow$ the best learning rate.\n",
    "    * Once its norm becomes smaller than the ***tolerance*** $\\epsilon$, the gradient vector becomes tiny $\\Longrightarrow$ approxiamte minumal Gradient Descent $\\Longrightarrow$ ***Interruption!***\n",
    "    \n",
    "#### 1.2.6 Convergence Rate\n",
    "When tjhe cost function is convex and its slop does not change abruptly (e.g., MSE) $\\Longrightarrow$ Batch Gradient Descent with a fixed learning rate's Convergence rate is $O\\frac1{iterations}$.\n",
    "    \n",
    "#### 1.3 Stochastic Gradient Descent (SGD)\n",
    "It picks a dandom instance in the training set every step and computes the gradients based only on that single instance $\\Longrightarrow$ faster but bounce up and down, descreasing only on average $\\Longrightarrow$ never settling down $\\Longrightarrow$ ***good final parameter values, not optimal***\n",
    "\n",
    "***Since it is jumping around, it may jump out of the local minuma to find the global optimal. However, it never settle at the minimum*** $\\Longrightarrow$ ***gradually reduce the learning rate（simulated annealing）***.\n",
    "    \n",
    "#### 1.4 Mini-batch Gradient Descent\n",
    "* Computing the gradients on small random sets of the instance mini-batches.\n",
    "* Batch-GD: entire dataset\n",
    "* Stochastic-GD: instance by instance (one each time)\n",
    "    \n",
    "### 1.5 Comparision of the algorithm for Linear Regression\n",
    "Algorithm     | Large m | Out-of-core support | Large n | Hyperparams | Scaling required | Scikit-Learn |\n",
    "------------- |:-------------:| :-----:|:-----: |:----: | :----: | :---: |\n",
    "Normal Equation| Fast | No | Slow | 0 | No | LinearRegression |\n",
    "Batch GD | Slow | Slow | No | Fast | 2 | Yes | N/A|\n",
    "Stochastic GD | Fast | Yes | Fast | >=2 | Yes | SGDRegressor |\n",
    "Mini-batch GD | Fast | Yes | Fast | >=2 | Yes | N/A |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **From Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the sum of the squared elements in v\n",
    "def sum_of_squares(v):\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "# to maximize the sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-74c330de92c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# random initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# simple implementation\n",
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "# Implementation\n",
    "n_epochs =  50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t + t1)\n",
    "\n",
    "theta =  np.random.randn(2,1) # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradient =  2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To make sure the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it agin, and so on. $\\Longrightarrow$ ***SLOW***!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.3.1 Scikit-Learn SGD\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(n_iter = 50, penalty = None, eta0 = 0.01)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix}\n",
    "    3x_1 - \\cos(x_2x_3) - \\frac{3}{2} = 0  \\\\ \n",
    "    4{x_1}^2 - 625{x_2}^2 + 2x_2 - 1 = 0   \\\\\n",
    "    \\exp(-x_1x_2) + 20x_3 + \\frac{10\\pi-3}{3} = 0\n",
    "    \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforme to associated function $G(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G(x) = \\begin{bmatrix}\n",
    "    3x_1 - \\cos(x_2x_3) - \\frac{3}{2}         \\\\\n",
    "    4{x_1}^2 - 625{x_2}^2 + 2x_2 - 1          \\\\\n",
    "    \\exp(-x_1x_2) + 20x_3 + \\frac{10\\pi-3}{3}\n",
    "    \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $x = $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
