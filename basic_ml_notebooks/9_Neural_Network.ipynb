{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **Neural Network**\n",
    "---\n",
    "## **Summary**\n",
    "to be added ...\n",
    "    \n",
    "## **References**\n",
    "1. Goodfellow, I, Bengio, Y., Courville, Aaron. 2016. Deep learning. The MIT Press.\n",
    "2. GÃ©ron, A. 2017. Hands-Machine Learning with Scikit-Learn & TensorFlow. O'Reily Inc.\n",
    "\n",
    "## **Concept**\n",
    "<font size=4 color=red>**1.1 The Basic Arhitecture of Sinlge Layer (Preception) Neural Network**</font>                                   \n",
    "#### <font color=blue>A Neuron really computes two functions within a node</font>\n",
    "* Summation $\\sum$ and activation $\\Phi$                                            \n",
    "* <font size=3>For a situation where each training instance is of the from ($\\overline{X}, y$), where $\\overline{X} = [x_1,...,x_d]$ containg $d$ features, and $y\\in${-1, +1} contains the _observed value_ of the binary class variable.</font>\n",
    "<font size=3>The input layer contains _d_ nodes that transmit the _d_ features $\\overline{X}$ with edges of weight $\\overline{W} = [w_1,..., w_d]$ to an output node, then $$\\hat{y} = sign\\left\\{\\overline{W}\\cdot{\\overline{X}}\\right\\} = sign\\left\\{\\displaystyle{\\sum_{j=1}^{d}w_jx_j}\\right\\}$$ or with ___bias___(_b_) $$\\hat{y} = sign\\left\\{\\overline{W}\\cdot{\\overline{X}} + b\\right\\} = sign\\left\\{\\displaystyle{\\sum_{j=1}^{d}w_jx_j} +b\\right\\}$$ sign function ***maps*** a real value to either -1 or +1 for binary classification. The erro of the prediction is therefore $E(\\overline{X}) = y-\\hat{y}$, which is one of the values drawn from the set {-2, 0, +2}.</font>                                              \n",
    "#### The ***bias*** can be incorporated as the weight of an edge by using a ***bias neuron*** through:\n",
    "* Adding a neuron: transmiting a value of 1 to the output node\n",
    "* Feature engineering trick: additional feature with a constant value of 1\n",
    "\n",
    "#### **1.1.1 Loss function**: to minimize the error in the prediction\n",
    "$$Minimize_{\\overline{W}}L = \\displaystyle\\sum_{(\\overline{X}, y)\\in{D}}(y - \\hat{y})^2 = \\displaystyle\\sum_{(\\overline{X}, y)\\in{D}}\\left(y - sign\\left\\{\\overline{W}\\cdot{\\overline{X}}\\right\\}\\right)^2$$\n",
    "* The sign function is non-differentiable; taking on constant values over large portion of the domain $\\Longrightarrow$ zero values at differential points\n",
    "* A staircase-like loss surface $\\Longrightarrow$ **NOT** suitable for gradient-descent.\n",
    "* Solution: a smooth approximation defined by *preceptron criterion* (<font color=red>NOT true gradient of the staircase-like surface</font>) $$\\nabla L_{smooth} = \\displaystyle\\sum_{(\\overline{X}, y)\\in{D}}(y - \\hat{y})\\overline{X}$$\n",
    "\n",
    "#### **1.1.2 Choice of Activation Function $\\Phi(\\cdot)$**\n",
    "* Pre- vs Post-activation\n",
    "    * Pre-activation: value computed before applying the activation function\n",
    "    * Post-activation: value computed after applying the activation function\n",
    "* Activation Functions\n",
    "    * <font color=red><b>Identity or linear: $\\Phi(v) = v$</b></font>\n",
    "        * used in the output node when the target is a real value\n",
    "        * used for discrete outputs when a smoothed surrogate loss function needs to be set up\n",
    "        * <font color=green>derivative is 1: non-differentiable</font>\n",
    "    * <font color=red><b>Sign: $\\Phi(v) = sign(v)$</b></font>\n",
    "        * map to binary outputs at prediction time only; non-differentiability prevents from creating the loss function at training time, and only linear activation during the training\n",
    "        * <font color=green>derivative is 0 at all values of _v_ other than *v* = 0 $\\Longrightarrow$ zero gradient and non-differentiable; rarely used in the loss function</font>\n",
    "    * <font color=red><b>Sigmoid: $\\Phi(v) = \\frac{1}{1 + e^{-v}}$</b></font>\n",
    "        * outputs in (0, 1)\n",
    "        * interprted as probablity dervied from maximum-likelihood\n",
    "        * historical tool for incorportating nonlinearity in neural network\n",
    "        * <font color=green>derviative $\\frac{\\partial{o}}{\\partial{v}} = \\frac{exp(-v)}{(1+exp(-v))^2} =  o(1-o)$: often used as a function of the output.</font>\n",
    "    * <font color=red><b>Tanh: $\\Phi(v) = \\frac{e^{2v} - 1}{e^{2v} + 1}$  $\\Longrightarrow$ $tahn(v) = 2\\cdot{sigmoid(2v)} - 1$</b></font>\n",
    "        * outputs rescaled to [-1, 1]\n",
    "        * similar to sigmoid, but its mean-centering and larger gradient $\\Longrightarrow$ easier to train.\n",
    "        * historical tool for incorportating nonlinearity in neural network\n",
    "        * <font color=green>derviative $\\frac{\\partial{o}}{\\partial{v}} = \\frac{4\\cdot{exp(2v)}}{(exp(2v) +1)^2} =  1-o^2$</font>\n",
    "    * <font color=red><b>Rectified Linear Unit (ReLU): $\\Phi(v) = max\\left\\{v, 0\\right\\}$</b></font>\n",
    "        * easier to train multiple layered neural network, replacing sigmoid\n",
    "        * modern neural network activation function\n",
    "        * <font color=green>derviative: 1 for non-negative values of its argument, and 0, otherwise.</font>\n",
    "    * <font color=red><b>Hard tanh: $\\Phi(v) = max\\left\\{min[v, 1], -1\\right\\}$</b></font>\n",
    "        * easier to train multiple layered neural network, replacing sigmoid\n",
    "        * modern neural network activation function\n",
    "        * <font color=green>derviative: 1 for the argument $\\in[-1, +1]$ and 0, otherwise.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAAD9CAYAAAA71evxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8U1XeP/DPSdJ0pRt7KSBdqFCBqq2CjoosIi6dcUTFBx1cGZ/BR/GZUZlReXRGR1xm3NBxUEZwGdHR+QHjgoDghkIHhoIiS4ECbWmBtulK2zTJ+f1x0/SmbUqbJrk3yef9evGiSW5yT5J+e+793nO+R0gpQURERERERERE+mbQugFERERERERERHR6TOIQEREREREREQUBJnGIiIiIiIiIiIIAkzhEREREREREREGASRwiIiIiIiIioiDAJA4RERERERERURAI+ySOEOIMIYQUQpj89PoNQog0f7y2FoQQrwohHtG6HURthBBzhBDrtG6HvzDmSE+0irfT7VcI8YUQ4o5Atqk3hBAXCSH2ad0OIm8JIe4SQmzQuh29IYRYLoR4QOt2EJE7IcRuIcRkrdsRzMIiiSOEOCyEmBaA/XQ6iJRSxkkpDzkfXy6EeLwPr39YCNHkTAy1/Uvpa7u72d8tQohv1PdJKe+SUv7BX/sk6ooQ4idCiG+FELVCiGohxGYhRB4ASCnfkVJe5qf9MuYo7GgVb93py36FEI8KIVo7xLFfT+ycF4cy2m5LKb+WUmb5c59EHX7HHR36rzkBbkuFEOJUhzb19+P+OiWZpJS3SCmf9tc+iXqqw/FkhfOcMK6Hz50shCj18Finc8/TbN82eEEdlzt7/456rqvzXylltpTyC3/uN9T5ZfQJ+dXVUsqguhJC1BdCiHgAHwH4bwDvAzADuAhAS4CawJijsKGDePOX96SUN2ndCCJ/klK6TgqFEIcB3KFx/3WZlPKb029GFBaullJuEEIMAfAZgN8CeEijtiRKKW0a7Zt8ICxG4qgJIYxCiGeFEJVCiEMAruzweIIQYpkQolwIUSaEeFwIYXQ+dosQ4hvn8y1CiGIhxEznY09AOdBd4sxqLnHeL4UQGUKIeQDmAHjA+fi/hBD3CyE+7LD/l4QQz/fyPXXKuKpHHzmvQr4vhHhTCFHvHMKWq9p2uBDin0KIk0KIKiHEEiHEGACvApjkbG+Nc1u3bKoQ4k4hxAHn1do16lEKzvd+lxCiyPl5vSyEEL15b0QARgOAlPJdKaVdStkkpVwnpdwFdB69IoS4TAixzzmK4BUhxJdtVym6i+HeYMxRCPN1vG0WQjwnhKgRQhwSQlzgvL9ECHFCCDFX9VoJzpg5KYQ4IoR4WAhh8LDf6UKIvc79LgHg1e+56DBS1xm7bzt/brtiOVcIcVQoxw0PqbY1CiF+J4Q46Izz7c7Y/sq5yU5nLN/Q8W+GEGKMUK6g1jj/PuSrHlvujN2Pna+7VQiR7s37I1ITQlzo/H2qFUIcc8amyflYlPP3/U7n77RFCPFc55cQLzp/bw8KL0a5CyEuF0Ic6HBfhRDiJ86fFwsh3hFCvOv8/d8lhMhRbXuGEGK1Mx4rhRB/EkKcDeB5AJOdMVfh3HalEOJh1XPnO9td5eyDB/fivRP5hJSyAkoSR/17HSmUY9OjQojjQplKHx3Idqn7P+dtt5Ijzj7rD85+vV4IsU4IMUC1fdso3hpnH3+L6OL817mt+pg5UgjxvPNv0jHnz5HOxyYLIUqFEL8WyjFDuRDi1kB+LnoVdkkcAHcCuArA2QByAczq8PgKADYAGc5tLgOgHqZ2PoB9AAYAeBrAMiGEkFI+BOBrAHc7p1DdrX5RKeVSAO8AeNr5+NUA3gZwuRAiEQCcQXIDgLd8+H7b5ANYCSARwBoAbUkmI5SrrkcAnAFgGICVUso9AO4C8J2zvYkdX1AIMQXAkwCuBzDU+RorO2x2FYA8ABOc283w9RujkLcfgF0IsUIIMVMIkeRpQ2dn8gGUqxv9ocTqBR026zKG/dBuxhwFI3/E2y7n43+H8vuaB6WPvQnKhY+20QMvAUgAkAbgEgC/ANDpYM253w8BPAwljg8CuNCbN9tDPwGQBWAqgEVCSbgCwP8CuBHAFQDiAdwG4JSU8mLn4xOcsfxeh/ZHAPgXgHUABgH4HwDvCCHU061uBPAYgCQABwA84Y83RmGnFcDdAJKhXHi8Gu7HuAAwE8rx7zkAbhXudSsuBrANSjwvAfC6n9p5DYC/Qek/P4eSoGmLnU8B7AEwAsBwAB9KKXcAWADgC2fMDen4gkKIKwA84nztYQAqoRyHq3X33ol8QgiRCuV3TZ3MfArKRZQcKP3jMACLAt+60/ovKP3yICgjdX8DAEKIEVBi8yUAA6G8j0IP578dPQRgovM5EwCcB6V/bzMEyrHBMAC3A3i5u2OTcBGOSZzrATwvpSyRUlZDOSECADgz8jMBLJBSNkopTwB4DsBs1fOPSClfk1LaoSR8hgIY7E1DpJTlAL4CcJ3zrssBVEopt3fztFXODGeNEGJVL3b3jZTyE2e734ISJIASKCkA7ne+5+ZeDH2dA+BvUsr/SClboBzITxJCnKHaZrGUskZKeRTAJqiyzkQ9IaWsg3ISJQG8BuCkUEagdBV3VwDYLaX8p3OY6IsAKjps09sYZsxR2PBDvBVLKd9wxsF7UE66fi+lbJFSrgNgBZDhTG7eAOC3Usp6KeVhAH8CcLOH/f4opfxAStkK5QSv4347ul4VxzWid7WtHnOOSNoJYCfaY/kOAA9LKfdJxU4pZVUPXm8igDgosWqVUm6Ekti9UbXNP6WUBc7P9R0wjskHnL9T/3aOsjsIJQlzSYfN/iilrJNSFkM5RlX/7u2TUr6p6j9Htl2I9OBTVcx1vODQnY1SyvWq/rOtDT+BkjD9nZTylDMuv+3ha84BsFRKuUtK2QzgAQBThTK1pU13752or1YJIeoBlAA4AeD/AGV4G5RBBvdJKaullPUA/gj3809fq1TF5m968bw3pJT7pZRNUKZct8XIHAAbnKN4W6WUVVLKwh6+5hwoxwUnpJQnoVzAUPf9rc7HW6WUnwBogHJhJayFYxInBUrwtDmi+nkkgAgA5W2/2AD+CiXb2MZ1oCilPOX8sUeFqTxYAeVqJJz/n24Uzs+klInOfz/rxX7UB7inAEQ5R/4Mh3JS6828yBSoPj8pZQOAKiiZUk/77ctnRWFKSrlHKgUKUwGcBeV3r6tph27xLaWUADoWd+ttDDPmKKz4ON6Oq35ucm7X8b44KCNqzHDvk4/A/Xe7u/2WdLGd2vuqOE6UUh47zfZqnmJqOJRRQL2VAqBESulQ3dfxvTKOyeeEEGOFEJ86p2vUQbnSP6DDZt397nV8DOj+d3OmKuZ6c0LaXcwVd4idnurYf9YAqAPjjgLnZ1LKfgAmAzgT7bE3EEAMgO2q88+1zvtPxwbl3FUtAkriozsDVLH5bE/fAHzfHwIdYtP5s/pCS1WHY2bGJsIziVMO5RetzQjVzyVQijeqf7HjpZTZPXxt6cXjqwCMF0KcBWUaxDs93JdaI5TgB+CartGTwAeU9zxCdL3E+unezzEoia+2/cZCGWJb1sN9E/WalHIvgOVQTi47KgeQ2nbDeXUjtYvt+ooxR2EhgPFWCeWgc6TqvhHo+nfbrR937nd4F9v1hFssQxm23VMlALypVXMMwHDhrPfj5Om9EvnSawD+AyBdShkP4Pfwsp5UH3TsPyOgTO/qiRIAZ3SInTa97T8ToIzqYdxRQEkpv4TSr7YlTyqhXNDIVp1/JkhVofJuHIUyNV9tFNyTIj3lr/6wV7EJpT/szYWWsBSOSZz3AdwjhEh1zqdb2PaAVKY3rQPwJyFEvBDCIIRIF0J0HGrqyXEoc/l7/LhzSOcHUGoFFDinQPTWfihX+a90doYPA4js4XMLoBwQLxZCxAqluFtbbYHjAFKFEGYPz/07lDnDOc4CVH8EsNU5DJ7IJ4QQZzoLmqU6bw+HMu1gSxebfwxgnBDiZ84kyXz0rhPqKcYchSSt4s05beJ9AE8IIfoJIUZCqTnTsWZF236zhRA/d+73Hm/3C6AQwGwhRIRQio93rJPXndcB/EEIkSkU40X7EsrdHQ9shXKw/IBzv5Oh1CbpzXQTIm/0A1ArpWwQQmRDmcIRaHsAJAshpjr7z8fQ8/ORbwDUQ4m7GCFEtBCirQ7XcSjJ0Y6jEtq8C+BOIcRZQogoKDVINkqlyCxRoD0PYLoQIsc5suw1AM8JIQYBgBBimBDCraah83hR/U9AmaZ8qxDiPGc/NBrAffCuPykEcLEQYoQzyfnbXjz3HQDThBDXCyFMQoj+or0g+enOj98F8LAQYqBQat4tQtd9P6mEYxLnNSgVwXdCuRrxzw6P/wLKkO4fAVigJFiG9vC1XwAwSyhV7V/s4vFlAMaKzrU1VgAYBy8LGkspawH8CsoBZRmUg8OOQ9o9PdcO5eAxA0o2txRKXQIA2AhgN4AKIURlF8/9HEqRuA+hnJSmw7/zNyk81UMpjrpVCNEI5WTyBwC/7rihlLISSo2pp6FMMxoLpQijT5dHZsxRCNMy3v4HSiwdgnKy9ncoxU097Xexc7+ZADZ7uc9HoMSRBcrJ5N978dw/Q0k8rYMyLWMZgLbVRB4FsMLZ31/fof1WKIXPZ0K5AvsKgF84Rz0R+dN9AO4QQjQAeBnKCWBAOeP3XignfaVQpmd06u88PLcVSk2sCc7nHgXwc+fDawEcBnBCdFg90vncj6DUwVwD5Sr/EHRdc4vI75y1X96E0gcBwINQCh1vcU513AD3ui/DoIzWUf9Ll1J+BmVAwhsAagF8AuW8cqkXbVoP5W/CLgDbodRq6+lzj0KJzV8DqIaSEGqrH+fp/LfN41COHXYB+B7K+fnjXWxHKkKZSk5aEkpF770AhkilqCQR+YBzyHUpgDlSyk1at4colDHeiIiIiPwvHEfi6IrzoPd/oSwxzAQOUR8JIWYIIRKd041+B2W+f1dTQYiojxhvRERERIHFJE4fCCH+JoQ4IYT4wcPjQgjxohDigBBilxDiHNVjc4UQB6BUFZ8F5zJzRNRnk6AMz26CclL5M+dSiC49iM0i57+5gW06UdCZBGVFikoo0wQ7xZuaD/pNxiaRHzA2ifSHcUmecDpVHwghLoayVv2bUspOK3cIIa6AMsf/Cig1Bl6QUp4vhEiGMvcvF0rF7u0AzpVSWgLWeKIQxtgk0ifGJpE+MTaJ9IdxSZ5wJE4fSCm/glK8yZOfQgk6KaXcAiBRCDEUwAwA66WU1c5gWg/gcv+3mCg8MDaJ9ImxSaRPjE0i/WFckiemXm7PYTsdFBcX46qrrgK6+GyuvPJKLFy4cB6cq11MmTIFTz311LEvvvgCzc3NADAPAH7/+98jOjratZ3a0qVLsXSpUmB87969aGho8KqdrY0OvD6uDABgMAF3/JAKo1l49VphTM8fGGOzg0DE5lcvHsGFp+7C/Ucm4JmRO/35dsLakHPNuOYfg7vbRK+xybjsQrD0m6HI1uxA43EHGo/bXP+fOm5HU5UDzTUOtNQ60GxR/m+pdUA6un+90dfEYOqf+ne3CWMziDA2FVJK1JfacbzQiup9VlTta4XlgA0Nx2xwtGrdup4Zf2scLnwkqbtNGJtBIpjisvC1Onz3ZC0AIHtOLC7+Q7LXrxXGehSbvU3iUC90NVVNCOHx/q7MmzcP8+bNAwDExsZ63RbLQZvr5/iRJiZwKKz5KjbPN9Xj28dr0CqbYY4TiOpv9HlbCYgdws81XOip3wxWraccqC5qRW2xDbVHbKg7YkPNYeX/ZstpsjK93Vcjz7fCRajHZmujA0c2NePIxiYc29qChnJ7QPYrDIAwAgajgDC23RYQBgDCeTYnANdHKpyfr4DrVK+rbSITOdkiHOgtLi1F7eebSRkRfXot6h6TOH6UmpqKkpIS1+3S0lKkpKQgNTUVX3zxhdv9kydP9mtbLAfbLx0kpTOoKLz5IzazZsXiJ4u6vepFRKehp34zGLTUOlDxnxZU7raick8rqva0ovaIzWfXss1xAuZ4AyLiBMyxBkTECkTEtN/uP5bHE+EiFGNTSoljW1uw++0GHP68GfaW0wdOZKIBcUONiBtqRPQAI6ISDYiMNyAy0YDIBCVGTFHOf5ECxqj22waTgDABBoMzYWP0fGJN1BN6i0uebwYOkzh+lJ+fjyVLlmD27NnYunUrEhISMHToUMyYMQO/+93vYLEotaXWrVuHJ5980q9tsRxQBVUGv3YKb3qKTSJqx9js3qlKO0o3N6P83y2o2GZF9f7eze0wmICYQUbEDnb/Fz3AiKgk5SS07f/IBAOMETzBJEUoxaaUEsXrmrDthTpU7e06hiJiBYacE4kBYyOQnKX8SxhpQkQMR7iQfugpLqWUbuebiTzf9Ct+un1w44034osvvkBlZSVSU1Px2GOPobVV+eW96667cMUVV+CTTz5BRkYGYmJi8MYbbwAAkpOT8cgjjyAvLw8AsGjRIiQn+3fOYM1BDm+j8BFMsUkUThibvSOlxMnvW3FkUxOObmrGie+tpx1lI4xAYpoJSenKSWf8GSYkjFT+xQ42QhiYmKHOwiU2T+624ptHLajYbu30WHJWBNIuj8bIyVEYkG2GwcRYIW0FU1yeOumAtV7poMxxArGDORXen3q7xDgnP2soNjYWjY2NXj3371PLUVusJHJmrRmMgWeZfdm0cKHn3pyxqYGdy+rx7RM1WHA4C58/WsbpVNrRa2wyLjXWl35TS9X7W1H0r1M48K9G1B31XJtDGIGB2WYMyjFjwJgI9B9jRvJoE0xRuhktwNikLgU6Nu2tEv95pQ7/ebkOjvbrmjBFC4y+JgZn3RyH/llhdWzM2KRO+hKXpZub8a+bTwIABuWYce0/u12QgjxjYWNS2K0SdUfbe6zENH7tREREetLa5EDRmlPY/XYDKnd3PcVDGIDB55iRemEUhuZFYnCOmdM7iE6jqdqOdXdX4diWFtd9hgjgrJvicM78eEQnc8QAUV+518Phuaa/8RMOA7WHbZDOC3n9hhl5wEfkB6xNSETeqC+zYdcb9dj7QSOsdZ0vQpv7CZwxLRojp0Rj+E+iEJnAPpyop2qP2PDR3BNuI9qGnGvGpU8lIzGN5QWIfMVygKU7AolJnDDgXtSYQUVERKS1uhIb/vOXOuz7sBGODgNvjJECZ0yNQsbVMRgxORqmSGaJiXqrprgVa+acRGNFewInb0E8zpkfD4ORMUXkS24jcXi+6XdM4oQBrkxF5CecuU1EvXTqpB0Fz9Vi7z8aXaNk28SPNCF7TizOnBWLqERO8SDyVkOFzS2BY4oSmPZ8MkZdFqNxy4hCk6WISZxA4hl9GFAPb0tkUBEREQWcvUVi1/J6bH+5Dq0N7hngIblmnPOreIy4OIqrRxH1UespBz69s9ItgTPz9QFIvSBK45YRhaaWWgeaKh0AAKMZ6JfKixD+xiROGHAvNMUkDhERUSCVbWnGl7+1oPaIze3+lEmRyP2feKScHwnBwlpEfSalxMb7q13FwQ0mYOZSJnCI/Ek96yMxLYLTFQOASZwQ57BL1BxSF5riV07kF+yviKgDa4MDW56uxe63G9zuT8ow4YKHEzHi4miNWkYUmva834hDnza5bl/0hySk/oQJHCJ/YumOwOOnHOIayuywtyjDtqMHGDjHnoiIKAAqtrdgw4Iq1Je1F74x9xM479cJGHtjHIwRzPwS+VJNcSs2/77GdTt7TizG3hCnYYuIwoPlIFemCjQmcUJcNYtMEfmNZGVjIupAOiQKX6/H1mdq3QoXj5wahUseT0bsYF5MIfI1KSW+fMgCW5PSLyemmzDpd4kat4ooPLCoceAxiRPi3Ovh8OsmIiLyl5Y6Bz6/rwpHNjW77otMMOAnjyYiMz+GdW+I/OTAv07h2JYWAIAwAtOe64+IaIPGrSIKD+rzzUSebwYEP+UQV3OAw9uIiIj8rfZwKz65sxI1qmHlg882Y/qL/dFvGA+3iPyltdGB756sdd0ef0scBp5l1rBFROGjtcnhmjYsDEDiGTzfDAQeVYQ4t5E4TOIQ+Q0vsBOFr2Nbm7H2v6vQUuNw3Tfhzn44/zcJrH1D5Ge7ljeg8bhyEhkz0IDcexI0bhFR+Kg5ZENbdYH4kSYYI9nnBQKTOCFMSsk5ikRERH508JNT2HBfFRzO7tYYKTDlmWRkXBWjbcOIwkBLrQOFS+tct8/7dQLM/TiNiihQeK6pDSZxQtipEw5YG5TUqDlOIGYQOzUin2JdY6Kwtuf9Bnz5OwukcwBO9AADZi4dgME5kdo2jChM7FxWD2u90hknnGFC1s9jNW4RUXhxW5mK9XAChp90CLMcUBWZyohgQUUiIiIf2fm3enz7ePtyxolpJly1YiDr3xAFSGujA9+vqHfdzr03HgYTj3WJAkl9vsmROIHDI40Qpq6Hk8ygIiIi8omOCZwB2RG4avlARPfn8uFEgbL3w8b2UTgjTZzCSKSBGtZf1QSTOCFMPUcxMYNfNZFfcaQbUVj48d0GtwTOkHPNuGLZQETGc8oyUaBIh8T3bzS4bo+/NQ4GI/thokCyt0rUHuZ0Ki3wkw5h7nMUmRklIiLqi/2rGvHlwxbX7SG5Zly1fCAiYpjAIQqkwxubUXtEOc41xwtkXctaOESBVnfEBofzdDNuqBERsewLA4WfdB+sXbsWWVlZyMjIwOLFizs9ft999yEnJwc5OTkYPXo0EhMTXY8ZjUbXY/n5+X5pH+coUrgKVGxKFjYm6hW995vdKd7QhI2/qXYVNB84LgJXLmMCh0JDsMXmj++0j8IZOzuOJ48UsvQcmzzX1A5H4njJbrdj/vz5WL9+PVJTU5GXl4f8/HyMHTvWtc1zzz3n+vmll17Cjh07XLejo6NRWFjot/a11DrQVKksl2E0A/1SOU+fwoPeY5MoXAVzbJ7Y2YIN91S5VqFKzorAVSsGciljCgnBFpuNx+0o+brZdTt7TlzA9k0USHqPTfWsj0ROpQooHn14qaCgABkZGUhLS4PZbMbs2bOxevVqj9u/++67uPHGGwPWPreVqdIjOE+YwoZmsckQI+qW3vtNT+pKbfjkzkrYmpUhOPEjjLj6zYGISuTFEQoNwRab+/9foyuhmjIpEvHDefJIoUnvsak+30zO5EicQGISx0tlZWUYPny463ZqairKysq63PbIkSMoLi7GlClTXPc1NzcjNzcXEydOxKpVq3zevmr18DbWw6EwovfYJApXwRibLbUOfHLbSdfI1sgEA67420DEDGQCh0JHMMWmlBJ7P2h03T6TtXAohOk9NjsOGqDAYeraS7KLYhjCw+o0K1euxKxZs2A0th/0HT16FCkpKTh06BCmTJmCcePGIT09vdNzly5diqVLlwIAbDZbp8c9qXGbo8ivmcJHIGPzuxdO4DzM9V3jiUKY3vvNjhx2ifX3VsFyQHkNgxm4/K8DkJTGA1UKLcEUm8cLrag5pDw3IlYg7fJor16HKBgEIja9jUvpkKhRL6LD882A4kgcL6WmpqKkpMR1u7S0FCkpKV1uu3Llyk5D29q2TUtLw+TJk93mL6rNmzcP27Ztw7Zt22Ay9Tw42g46ARaaovASyNi85557fNRqotCn936zo4I/16Lkq/a6G1OeTkbKeZFevx6RXgVTbB78+JTr5/SZMSwsTiEtELHpbVzWH7O7phlHJRsQncwRqoHEv3xeysvLQ1FREYqLi2G1WrFy5couq37v27cPFosFkyZNct1nsVjQ0tICAKisrMTmzZvdClT5guUgq4VTeNJ7bBKFq2CKzYOfnsKOv9S7bp8zPx6Z+Zy2QaEpWGJTSolDa5tct9Ov4igcCm16jk2uTKUtjnvykslkwpIlSzBjxgzY7XbcdtttyM7OxqJFi5Cbm+sKsHfffRezZ892G/q2Z88e/PKXv4TBYIDD4cDChQt9GlStpxyoL7UDAIQRSDiDXzOFD61i08PoViJy0nO/qVZd1IpND1S7bg+/JAp5C+L9si8iPQiW2Dy5y4qGY8rxrTleYNjEKL/sh0gv9BybbkkcrkwVcKKruXbd6NXG5FuxsbFobGw87XYnf7Dig/zjAICEUSb81+dD/d20cKHn03TGpgZ2/LUOW56qxYLDWfjij8cwaWGi1k0KV3qNTcalxnrab7axNjjwwU+Po7ZYmZIcP8KIWauHIDKBA5e9xNikLvU2NgFgy1M12PFXZYTc6J/HYOqz/f3RtHDB2KROehOXmxZWY+/7yrYXPpKI8bf282fTwkmPYpNHJSGIw9uIAoSHGkQh5etFFlcCxxQtcPmrA5jAIdKBTlOpZsZo2BoiqmHpDk3xyCQEWVgpnIiIqFf2/bMR+1e1F0295Ikk9D/TrGGLiKiN5YANtUfaV6VK/QmnUhFpRUrpvogOp1MFHJM4IchSxMwoERFRT9UcasVXiyyu21nXxmD0z1jImEgvjn7ZPgpn+EVRMEXqdTYQUehrqnSgpdYBQEmqxg7lylSBxiROCHJbmSqdSRwiIiJP7C0S6++tgu2UMj8yMc2Eix5N0rhVRKR29Itm18/DL+EoHCItuRc1jnArqEyBwSROiLG3StQd4fA2IiKinvjuqRpU7lYOSA1mYPqL/RERy8MjIr1obXSgfFuL6/aIi5nEIdKSOomTyNIdmuBRSoipO2KDw5nDiRtq5IEokR/1bnE/ItKbsi3N+H55g+v2Bb9NxICxrINDpCdlW1rgsCo/J2dFIG4oTxqJtORWf5WzPjTBM/wQ4za8LZNBRURE1JXWUw58sbDadXvkpVE46xdxGraIiLpy9Iv2ejgjOJWKSHM839Qekzghxn2OIq9UEBERdWXrs7WoO2oHAJjjBS75YzLn9RPpUMnX7fVwRkxmEodIazzf1B6TOCHGbbk3rkxFFDA89yMKHuXbWvD9ivZpVBc+nITYwVxdg0hvGo7ZXMlWU5TAkHMiNW4RUXhrqXPg1AllZSqDGYgfziSOFpjECTHqlakSOUeRiIjIja3ZgU0PVgPOmlbDL4lC1rUx2jaKiLp0rKC9oPGQc80wmnnFhEhLbueaZ0TAYGJMaoE64qVwAAAgAElEQVRJnBAiHRI1qkJTyZnMjBL5FQsbEwWdfz9fh9pipa+MiBOY/EQSp1ER6VTZlvYkTspETqUi0lqNWz0cnmtqhUmcEFJfZoetWTmrjOpvQFQSh4YTERG1OV7Ygp2v17tuX/DbRMSl8CCUSK+ObVUlcc7jVCoirXFlKn1gEieEuBeZYlARERG1sbdIbHqwGlKZyo9hF0RizOxYbRtFRB41HLOh7ohywmiKEhg03qxxi4jIUqQ632T9Vc0wiRNC1HMUWSmcKMA4G4NI17YtqYOlyHlCGCMw+UmuRkWkZ+p6OIPPMcMYyXgl0hrPN/WBSZwQ4rYyVSYzo0RERABw8gcrdrxa57o98YEErqhBpHPqqVTDJnIqFZHWbM0O1JUoq8UJA5CQxvNNrTCJE0I4nYoosCQLGxPpnt0qsemBakjluBNDz4vEWTfFadsoIjqt8n+3J3GGnseixkRaqym2uRb1iB9ugomj4zTDJE6IkFK6J3EyeIWRiIjoP3+pQ9VepX80RQlcujgJwsADTyI9a66xo+aQMsLcEAEMGs+Lk0RaU9fDSeS5pqaYxAkRTZUOWOuU1GhEnEDsEK5MRRRQPCck0h3pAP7zcvs0qvN+nYCEM3gySKR3J3ZaXT8PGGOGKYqnLERa48pU+sG/iCHCbRROWgSLNRIRUVhz2CTsrRIO5zHn4HPMGHcLp1ERBYPjO9qTOIPP5qpURHrgPuuDSRwtMYnTB2vXrkVWVhYyMjKwePHiTo8vX74cAwcORE5ODnJycvD666+7HluxYgUyMzORmZmJFStW9LktbkGVyeFtFN70FJtE1C6QsVm4tB5wLiduNAOXPpUMg5EXOIi6ord+U53EGZTDJA6FLz3FptvKVDzf1BQ/fS/Z7XbMnz8f69evR2pqKvLy8pCfn4+xY8e6bXfDDTdgyZIlbvdVV1fjsccew7Zt2yCEwLnnnov8/HwkJSV53R63lak4vI3CmN5ik4gUgYzN6qJW/PvFWtftvAUJ7BuJPNBbvykdEscL24saDzmbK1NReNJTbDpsErXFqvNNrkylKY7E8VJBQQEyMjKQlpYGs9mM2bNnY/Xq1T167meffYbp06cjOTkZSUlJmD59OtauXdun9qhH4iRyeBuFMb3FJhEpAhWbDrvEpger4XBeyB84LgIT7ujnq7dBFHL01m/WHLLBWq/UeYzqb0C/4azzSOFJT7FZe9QGh/N0M3aIEeZ+TCNoiZ++l8rKyjB8+HDX7dTUVJSVlXXa7sMPP8T48eMxa9YslJSU9Oq5ALB06VLk5uYiNzcXNputy22ADsPb0jnAisJXIGPzlVdecd1mGSqi7gUsNuetx4nC9qkYlz6dDIOJAUrkid6OaY/vUI/CMbPOI4WtQMRmT+Oy5gDPNfWESRwvSSk73dexk7n66qtx+PBh7Nq1C9OmTcPcuXN7/Nw28+bNw7Zt27Bt2zaYTF0HTEudA6dOKBP/jWYgfjgDi8JXIGPzV7/6lQ9aTBQeAhGbNYdaYfi2fZi5wQT0z2I9DaLu6OmYFgCOq5Kwg3M4lYrCVyBis6dx6Va6g7M+NMckjpdSU1NdmU4AKC0tRUpKits2/fv3R2Sk0vnceeed2L59e4+f2xvqqVQJoyJ4xZHCWkBjs4sOkoi65u/YlA6JTQurYW9R4nLA2AgYItgfEp2Ono5pAa5MRdRGT7HpNuuDSRzNMYnjpby8PBQVFaG4uBhWqxUrV65Efn6+2zbl5eWun9esWYMxY8YAAGbMmIF169bBYrHAYrFg3bp1mDFjhtdtcV/ujaNwKLzpKTaJqJ2/Y/OHtxpQsU05+TOYgMlPJfv5HRGFBj31m7ZmB6qLnMe1Ahg4jkkcCl96ik1LEc839YTfgJdMJhOWLFmCGTNmwG6347bbbkN2djYWLVqE3Nxc5Ofn48UXX8SaNWtgMpmQnJyM5cuXAwCSk5PxyCOPIC8vDwCwaNEiJCd7f7BpOciVqYja6Ck2iaidP2Oz7qgNW55uX43q7LviMTCbJ39EPaGnfrNqXyukXfk5cZQJ5jheb6bwpZfYlA4Jy6H2881Enm9qTnQ1X64bnDugodjYWDQ2Nna6/5PbT+LIpmYAwPQX+yPjqphANy1c6HlcPmNTA9uX1KLgz3VYcDgLXz1TjvN/naB1k8KVXmOTcRkA0iGx5uaTOPadUgw1abQJ160eAmOk8NhvUsAwNqlLnmJz9zsN+OoRCwAg4+oYTH+hf6CbFi4Ym9SJp7isL7Ph7YuUET+RiQbcuj2FBcf9p0cfLNPbIcB9jiIHVxERUfjY/U6DK4EjDMCUp5JhjOTBJVEwOrm7vR7OwLN4tZ9IDzqeazKBoz0mcYKcrdmBuhJl3KkwAImj2OERBQrrGhNpq67Ehu+eap9GlXNnPwyawNVsiIJV5Q/tSZwBnBJJpAuWIq5MpTdM4gS5mkM218DD+OEmXn0kIqKwIB0Smx6shu2U0gkmZZqQu4BTGomCld0qUbW//Yo/61oR6YPbSBzWw9EFJnGCnHplqkROpSIiojCx++0GHNvinEZlBKY8nQwTL2QQBS3LgVY4nANx+g03IjKBpylEesCVkPWHfx2DnHplqmQObyPSDKcHEwVO7ZEO06jmcRoVUbA7qZpKxVE4RPpRc5DTqfSGSZwgZylSj8RhUBERUWhzTaNqck6jGm1C3j2cRkUU7Cp3tx/Tsh4OkT40VdnRbHEAAEwxAnEpRo1bRACTOEGPK1MRaYiFjYkC7vs3G1BeoJ5G1Z/14IhCAFemItIft6lUaVyZSi+YxAliDptE7WHV8LY0dnhERBS6qve3YotqGtXZd8Vj0HhesScKdg67RNWPqpE4YxnXRHpgOcCpVHrEJE4Qqz1qg8PZ38UONcLcj18nERGFJnuLxIYFVbC3KEPg+p8Zgdy74zVuFRH5Qu1hG2zNSmzHDjYiZiCnbBDpgXrWRyJXptINnvUHMXU9nKR0TqUi0hRHlxL51dZna1C1V+n3jJEC057nNCqiUFG9r/2YNvlMnigS6YV6OlVyJs839YJJnCDG4W1ERBQOSr5uxs5lDa7bF/w2Acmj2e8RhQr3E0XGNpFeqM83uYiOfjCJE8Rq1EWNObyNKOAkCxsT+V1TtR0b769y3R4xOQrZN8dp2CIi8rXqIl7tJ9Iba70DjRV2AIAhAkgYwdjUCyZxgphbtXCuTEVERCFGOiQ23V+NUyeU5U2j+xtw6VPJXB2DKMSokzhJHIlDpAuWQ+1xmXCGCQYT+169YBInSEmHhOUQp1MREVHo2vFqPY5sanbdvvSpZBY8JQox9laJ2mIe0xLpDUt36BeTOEGqodwO2yllLkdUkgHR/XlQS0REoaNsSzMK/ty+nPiEO/th5JRoDVtERP5Qd6R9tdW4FCPMcTw9IdKDGrdZH0zi6An/SgYp9VSqRK5MRUREIaTxhB3r76mCVGZRYUiuGef/JkHbRhGRX1Tv51QqIj2qPsCVkPWKSZwgxeFtRDrAwsZEPmdvlVh/TxWaKtvr4Fz20gAYIzgXnygUuRc15jEtkV7U8HxTt5jECVKWgxzeRkREoUVKiW8etaC8oAUAIAzAtOf7I3YwpwwThSr35cV5tZ9ID2wtEnUlziSOABLTGJt6wiROH6xduxZZWVnIyMjA4sWLOz3+5z//GWPHjsX48eMxdepUHDlyxPWY0WhETk4OcnJykJ+f3+t9W4q4MhWRJ1rEJhfLITq908Xmi7euwY/vNrpuZ97mQOqFUQD63m8SkWdaHtNyZSoiz7SKzdriVteU5vjhRpiimDbQE579e8lut2P+/PlYv349UlNTkZeXh/z8fIwdO9a1zdlnn41t27YhJiYGf/nLX/DAAw/gvffeAwBER0ejsLDQq31LKWE5qBrels4Oj6iNlrFJRJ6dLjZLvm6G+Zuz258wpgxLC5/DVDA2ifxJy36TK1MReaZlbLqV7uC5pu4wpealgoICZGRkIC0tDWazGbNnz8bq1avdtrn00ksRExMDAJg4cSJKS0t9su+mKgdaapTUqClGIC6Fw8yJ2gQyNiVr4hD1WHexWb2/FevurgQcypC2QRPMOO+RSJSW+abfJCLPtDymrT3MlamIPNEyNt0W0WFyVXf4l9JLZWVlGD58uOt2amoqysrKPG6/bNkyzJw503W7ubkZubm5mDhxIlatWuXxeUuXLkVubi5yc3NhsykZ0ZqD7pXCBedxELkEMjZfe+013zSaKAx4is36Mhs+uuUkrPVKVjR2iBGX/3UA3njbd/0mEXmm5TGthVOpiDwKRGx66jMtB7kylZ7xG/GS7OISvKdkyttvv41t27bhyy+/dN139OhRpKSk4NChQ5gyZQrGjRuH9PT0Ts+dN28e5s2bBwCIjY0FAFQXcdgpkSeBjM2cxlpsf6nOd40nCmFdxaapNQYfzT2Jxgo7ACAiTmDmawPw/9a969N+k4g80/aYlitTEXkSiNj01Ge6TadibOoOR+J4KTU1FSUlJa7bpaWlSElJ6bTdhg0b8MQTT2DNmjWIjIx03d+2bVpaGiZPnowdO3b0eN/uI3EYVERqmsUmB8QRdatTbBZXIL3wRtQcUg4UDWbg8lcHYGf5Vz7vN4nIMy2PaS1FXJmKyBOtYtNhk6gt5vmmnjGJ46W8vDwUFRWhuLgYVqsVK1eu7FT1e8eOHfjlL3+JNWvWYNCgQa77LRYLWlqU5VMrKyuxefNmtwJVp6Oeo8iVqYjcaRmbROSZOjYbqpvR9NY4mE4OUB4UwLQ/98fJ6D2MTaIA07LfrFYf047miSKRmlaxWVdig92q/BwzyIDIeKYM9IYZAC+ZTCYsWbIEM2bMgN1ux2233Ybs7GwsWrQIubm5yM/Px/3334+GhgZcd911AIARI0ZgzZo12LNHOUg1GAxwOBxYuHBh75I4qpWpWGiKyJ2WsUlEnrXF5lWX/Qw/sz+JVIxzPZZwbQnSrxiOadMYm0SBplW/2WllKl7tJ3KjVWxyFWT9E13NtesG12LRUGxsLCwV9Vg2QSloZYgA7tydCoOJ8zgCRM8fNGNTAwXPKTVxFhzOwjfPlSPv3gStmxSu9BqbjMsOWuoc+OT2k6jYbnXdd8FDiZhwez+/7C82NhaNjY1+eW3qEcYmdSk2NhYlhTV4b0YFAGVlqpu/6TxNhPyGsUmdtPWZO16tw5anawEAZ/0iDhc9mqRxy8JKj2KTI3GCjLpSeMIoExM4REQUFBoqbPj41kpU72vvxy54OBETbvNPAoeI9M3CosZEusSVqfSP30qQcasUzuFtRLrhYbEAIoJSy+2juSfRUG533XfhI4kYfysTOEThqprLixPpkoUrIesekzhBxr2oMYOKiIj0reTrZqy/pwottQ4AgMEETF6cjKyfc/lvonDGlamI9EdKCcshnm/qHf9iBhm34W0MKiJtceY2kUdSShS+Vo+tT9dCKvkbmGIEZrzSHyMujta2cUSkObeROFyZikgXGivsaG1QDnDN8QLRA7gylR4xiRNk3KZTcXlxIiLSIWu9A18+VI0DHzW57osZZMDMpQMxaLxZw5YRkV5wZSoi/VGfayZnRkCwXoAuMQsQZOpLnIElgMRR/PqIiEhfyv/dgs9/XYX60vb6N0PONeOylwcgdpBRw5YRkV5IB+BwHtLGpRhhjuPVfiI9UJfuSGRyVbeYBQgi0gHXkPT44UaYotjhEekGL1RQmLO1SGx/qRY7Xq139VUAkD0nFhc+kgSjmUFCRAop2+cjc2UqIv1wr7/KVIFe8ZsJIuoOj/VwiIhIL0o3N+OrRyyoPdw+DNscL3DJE8nIuDJGw5YRkS6pEr1cmYpIPywHOc0xGDCJE0zUHR6TOESakyxsTGGu4ZgNW56pRdHqU273p0yKxNRnkhGXwsMMIupM3X9yZSoi/XBbRIcJVt3iX80gou7wktL51RERkTaaLXb85y/1+OHNetit7feb4wTO/00Csm+KgzBw+hQRdU095ZIrUxHpg5RAc5USnKZogX4prGOnV8wEBBF1h5fIkThEusLi/RQOTp20Y9fyeux+qwHWBvehaBlXx+DChxMRM5AHfUTkmd0qAbcLkzymJdIFVVwmppl4MUbHmMQJEg4bOzwiItJG1T4rvl/RgH3/bITD6v7YoAlmTHwwAcMmRmnTOCIKKuraWVyZikg/pENVf5XnmrrGJE6QqCtp7/BiBxsRGc8Oj4iI/Mfa4MCBj09hz3uNOFFo7fR4wigTzv9NAtIuj4bgUDQi6iH16jdcmYpIP9xKd3BlKl3jtxMkLAfakziJrIdDpA+sbEwhxlrvwOGNTTi0tgklXzbD1tz5d3zQBDPO/mU/nDE9GgYjkzdE1DvV+1WFU1kPh0g/uIhO0GA2IEior1owqIiIyBekQ6JqbytKv21G6eYWlH3X3Gm6FAAYIoBR06Nx1s1xGHpeJEfeEJHXqotUI3F4tZ9IN9xH4vB8U8/4lzNIuC33xpE4RPrDk1oKAq2NDpz8wYrjO604sdOKY1tb0Fzt8Lh98ugInHldLEb/LAbR/VmwmIj6zu3CJEfiEOlCa6PDVX/VYALiR/J8U8/47QSJGtV0qiTOHyYiom7YWyXqjthQXdSKmoOtqC6yoWqfFTUHbG4rHXZlwNgIpM2MQdrl0SxsSEQ+ZbdK1Barjmn5N4ZIFywH2+MyfqQJxghenNQzJnGCgJSyw0gcdnhEesCSOKQF6ZBornGgqcqBUyftaCizob7MjvoyG+qPKbcbjtnhsJ3+tQAgqr8BqZMiMezCKKReGIX4VB4aEJF/1B62uf42cWUqIv1wO9fkVCrd41/OPlq7di2ysrKQkZGBxYsXd3q8paUFN9xwAzIyMnD++efj8OHDrseefPJJZGRkICsrC5999pnHfTSW29HaqJwtRiYYED2AXxtRdwIRl0R9YW+RaDhmQ/X+VlTsaEHJ1804+Mkp7P1HA3a9UY9tL9Xiu8U1+PKhaqz7n0qs/q8TWHl5OZbnleGvo0uxPPcY3ptRgX/ddBKbHrRg24t12PfhKRz7rgV1Rz0ncIShfYrUxY8n4bqPB+OWrSmY/uIAjL0hzu8JHMYmkT4FKjbd6uFwZDnRaQUqNll/Nbjwclsf2O12zJ8/H+vXr0dqairy8vKQn5+PsWPHurZZtmwZkpKScODAAaxcuRIPPvgg3nvvPfz4449YuXIldu/ejWPHjmHatGnYv38/jMbONQfUw9uSMkwsKEnUjUDFJVFfHP2yCWvvqvLrPuKGGpGUGYGkdBOSMiKQlBGBAWMjEBGrzYUAxiaRPgUyNi1FrIdD3jn5vRUVO1q0bkZAORwSr/9+A169+zMkJSbi6WeewXpRhCFDh7i2+fqrr5FRPx0PL3od27dvx4u3rMGtt92KivIKfP9mA/7xu3+jtrYWS/57CYYsmgSDoevzyJKvm10/s/6q/vEb6oOCggJkZGQgLS0NADB79mysXr3ardNbvXo1Hn30UQDArFmzcPfdd0NKidWrV2P27NmIjIzEqFGjkJGRgYKCAkyaNKnTftSZ0UROpSLqVqDikqgvIvo4hcAcLxDd34jo/gb0SzEhbpix/f9hJvQbZkREjL5GbTI2ifQpkLHJlanIWyXfNGPrM7VaNyPgpuJe7F0CAPW4CHfhwF+BA6hxPS4wDqkYh28erQGQjkykO3+OwkW4C1sebwBgxFTci29/37PPjyNx9E/IXhR1GDBggGxqavJjcxQ2mw0mkz7+sHfXFpvNBofDAbPZ3OVtAGhqakJUVJRr9Ezb7dbWVhgMBtdrt7S0wGg0dtqXzWaDoxUwwAirbEKUORoGk/YjcYLlO/KlU6dOVUopB/p9R15gbLrf7++4dL1uq4QBJiU2I6Jh0EERuGD4jnxNr7HZXVxKh1LgEwAgAOH83/Nt552i64XQguF7D2Rs2mzKCFaHw4GYmBifv0dvBMN35GvBGJu+FgzfeyBjU7YKCBhglU2IjoqB0EGuORi+I18Lxth02CQcrV0+RD5ilU0wiyiYonUQmGBsdqdXSRy4Fh7zr9zcXGzbti0Quzqt7tryj3/8A5999hlef/11AMBbb72FgoICvPTSS65tsrOz8dlnnyE1NRUAkJ6ejoKCAixatAiTJk3CTTfdBAC4/fbbccUVV+Daa6/ttJ9VN5xA+b9bsOBwFvZvrMLIydG+fpu9FizfkY9pf4buGWPTKVBxCQBbnq7BjlfrseBwFr59qQLnzo/39dvstWD4jvxAr7EZsNLXwfC9BzI22xiNRtjtdl+9tT4Jhu/IDxibQfC9Byo27VaJ188qhcMGLDichfq6Bl0UNg6G78gPgi42S75uxuENvku+vv/++7j++ut99np94aktBw4cwNGjRzFlyhQAwL59+3D8+HFcfPHFrm3+/ve/Iz8/H3FxcQCU+L3uuuuwdetWDBkyBFlZWQCAjRs3YuTIkUhPT/fYjulPDsMlY6/CZ4Xv+/LteY2x6Zk+UltBKjU1FSUlJa7bpaWlSElJ6XKb1NRU2Gw21NbWIjk5uUfPbcNq4UQ9F6i4JKLeYWwS6VOgYlO9MpWE1EUCh4LH8IuiMPyiKJ+93n0fv4aXHvulz16vLzy1xfSdDSsfXYr/e0xJin7z5CYYAFz025+6tnl8yxokzjoHkyZNgs1mwzUv/x5/Xfw/2LZ4D05iD+747UTndktx2fxHMWlSksd2GJ8VqDId8u2bI7/gX88+yMvLQ1FREYqLi2G1WrFy5Urk5+e7bZOfn48VK1YAAD744ANMmTIFQgjk5+dj5cqVaGlpQXFxMYqKinDeeed12kdTtR3N1Q4ASofXL4VFHIm6E4i4JKLeY2wS6VOgYlNdD0cGbjAUUdBiv0me6HIkzrx587Rugkt3bTGZTFiyZAlmzJgBu92O2267DdnZ2Vi0aBFyc3ORn5+P22+/HTfffDMyMjKQnJyMlStXAlCGpV5//fUYO3YsTCYTXn755a5XpjrQvjKVEIDwUFE80ILlOyLf0tNn7aktgYjLruhl0bhg+I7I9/T0WespNvUylx4Iju+IfE9Pn7XWsVm9vz2JY9DRZeRg+I7I9/T0WWsdmz1pixbYFs90WROH2u3+ewO+etgCALivJAvNrac0blFY08lpepcYmxpQ18T5bkkFzvmV9jVxwpReY5NxqbHY2Fg0NjZq3Yxwxtgkl8/mV+LQp0pNk/8ty0JTC49pNcTYpE7YZ+pCj2JTR3lw6op6eXG9XOknIiceahAREfWIRTWdSi8jy4mIghGTODpXc7B9OhW/LSIiIiIKNnarRO1h9xIBRETkHd2nBZ599lkIIVBZWalZG+6//36ceeaZGD9+PK655hrU1NQEbN/qkThWawsWL14csH13paSkBJdeeinGjBmD7OxsvPDCC5q2x2634+yzz8ZVV12laTvCUbjHJgAcKm6v4P/ll18GdN9qeotLgLGpJcYmsHbtWmRlZaGpqUnTfpOxSW3CPS7VK1PVoQJNzYzNjhib2gj32Gyzdu1aNDU1ISMjg7HZgR5jU9dJnJKSEqxfvx4jRozQtB3Tp0/HDz/8gF27dmH06NF48sknA7Jfa4MDDeV2AIAwSkRFReHdd9/Fjz/+GJD9d8VkMuFPf/oT9uzZgy1btuDll1/WtD0vvPACxowZo9n+w1W4xyag/EFft26963bhzkLNYkFvcQkwNrXC2FRic/78+fj0008RHR2tab/J2CSAcQm4r0yVdt4gxmYXGJuBx9hUtPWbkZGR+PHHHxmbHegxNnWdxLnvvvvw9NNPQ2g85vKyyy5zrXAxceJElJaWBmS/NQfbO7zEURGAAGbPno3Vq1cHZP9dGTp0KM455xwAQL9+/TBmzBiUlZVp0pbS0lJ8/PHHuOOOOzTZfzgL99gEgIKCAiQlJbluT5gwQbPY1FNcAoxNLTE2ldjMyMhAWloaAG37TcYmAYxLwH1lqmETEgAwNtUYm9pgbCra+k2DwQCz2czYVNFrbOo2ibNmzRoMGzYMEyZM0Lopbv72t79h5syZAdmXRVUPJzE9AgCQmpqq6S+y2uHDh7Fjxw6cf/75mux/wYIFePrpp2HQ0zqVYYCxqSgrK0N8v/bVqBISEnQRm1rHJcDY1ApjU1FWVobhw4e7buul32RshifGpUJd1Dg5U1/HtIzN8MTYbMd+0zO9xqZJy51PmzYNFRUVne5/4okn8Mc//hHr1q3TRVt++tOfun42mUyYM2dOQNqkroeTlBHh+lnrbDEANDQ04Nprr8Xzzz+P+PjAL6v80UcfYdCgQTj33HPxxRdfBHz/oY6xeXpSdl6aSuvY1DouAcamvzE2T4+x2TXGpv8wLk/P7Zg2s/30g7HJ2PQnxmbPsN/smp5jU9MkzoYNG7q8//vvv0dxcbErM1paWopzzjkHBQUFGDJkSEDb0mbFihX46KOP8Pnnnwfsl1o9EicpQ/mqSktLkZKSEpD9e9La2oprr70Wc+bMwc9//nNN2rB582asWbMGn3zyCZqbm1FXV4ebbroJb7/9tibtCTWMzdNLTU3Ft/Xfo+1d19bWICVHu9jUQ1wCjE1/Y2yeXmpqKkpKSly3te43GZuhj3HZvY4rUyU7L0wyNhWMTf9hbPYM+82u6To2pZS9+aeJkSNHypMnT2q1e/npp5/KMWPGyBMnTgR0v+9ceky+MuqofGXUUVn42SEZExMjx48fL3/44YeAtkPN4XDIm2++Wd57772ataGjTZs2ySuvvDIQu+ptvATynybCNTallLK1tVXOHbVYvjLqqDSLaPmLrMc0i009xqWUjE2/vuPTCPfYHDVqlDx0SPt+k7GpeQzqKjbDNS4r97a4jmefHLWVsdkNxqY2wjU227T1m1FRUbKlpYWx2QW9xaa+Jnfp1N133436+npMnz4dOTk5uOuuu/y+T3uLRN1R51ULIXHj/Hw0NTXh+uuvR3Z2tt/378nmzZvx1ltvYatx7jwAABpCSURBVOPGjcjJyUFOTg4++eQTzdpD4U2L2GxjMpkwfdp01+3x48drFpuMS9IbrWNzyZIlmDFjhub9JmOT9ESruLQcaB+FM3hsHGOTqAMt+0ygvd9saWnBmDFjGJtBQEjZeQ5cN3q1MXmvap8V7888DgDol2rETV+lIDY2Fo2NjRq3LKxpX4zIM8amBr5bXIPCpfVYcDgLW16pwNl3aTNnl3Qbm4xLjbHf1Bxjk1DwXC22v1QHAJhwZz9c8NtExqb2GJvUCeNSF3oUmxyJo1M1bvVwIrrZkoiIiIhIn7pamYqIiLzHJI5OVRd1vTIVEemUXq9pERERaah6vyqJM5rHtEREfcUkjk7VHFQlcdI1XUSMiIiIiKjXbC0StUc6r7ZKRETeYxJHp9RF4DgSh0ifeldSjIiIKLzUHGqFtCs/x48wIiKGpx5ERH3Fv6Q65LBL1BzidCoiIiIiCl7V+ziViojI15jE0aH6EhvsVuXnmIEGRCbwayIiIiKi4MJ6OEREvsfsgA5ZVCtTJXIUDlFwYGFjIiIiN24LdXBlKiIin2ASxwvV1dWYPn06MjMzMX36dFgslk7bFBYWYtKkScjOzsb48ePx3nvvuR675ZZbMGrUKOTk5CAnJweFhYVuz7UcUBc1ZodH1FP+jk0i8g5jk0if/B2b6pE4/TkSh6jH2G9Sd5jE8cLixYsxdepUFBUVYerUqVi8eHGnbWJiYvDmm29i9+7dWLt2LRYsWICamhrX48888wwKCwtRWFiInJwct+e6JXFYxZ+ox/wdm52wsDFRjwQ8NomoR/wZm62NDtSXKFWNhRFITGMSh6in2G9Sd5jE8cLq1asxd+5cAMDcuXOxatWqTtuMHj0amZmZAICUlBQMGjQIJ0+e7NHrq6dTsagxUc/5OzaJyDuMTSJ98mdsqi9KJo4ywRjJecdEPcV+k7rDJI4Xjh8/jqFDhwIAhg4dihMnTnS7fUFBAaxWK9LT0133PfTQQxg/fjzuu+8+tLS0uO6XUrp1eh9veQ+5ubnIzc2FzWYDEXnmz9jsaOnSpXj77bd803CiEBfo2GS/SdQz/oxN9VSqpMwIxiZRLwSq32RcBichZa/mA4TN5IFp06ahoqKi0/1PPPEE5s6d6zZULSkpqct5igBQXl6OyZMnY8WKFZg4caLrviFDhsBqtWLevHlIT0/HokWLAAANFTa8dUE5AMDcT+C2wmEQQrlyERsbi8bGRp++T+oVPV9CYmz6OTa78u0fa7Dz9XosOJyFrX+tQM6d8X18d+QlvcZm2MQloK/YbMN+U3OMTR3QKja/fcKCncsaAAC598Yj794E12sxNjXH2NQBvfWbjEtd6FFssuCKBxs2bPD42ODBg1FeXo6hQ4eivLwcgwYN6nK7uro6XHnllXj88cddAQXAlVWNjIzErbfeimeffdb1WM0B96lUbQkcIlJoFZtE1D3GJpE+aRWb1UXtx7RcXpyoM/ab5C1Op/JCfn4+VqxYAQBYsWIFfvrTn3baxmq14pprrsEvfvELXHfddW6PlZcrI22klFi1ahXOOuss12OWg+qixuzwiHrDn7HZpd6NZCQKWwGPTSLqEX/Gpno6VXIWj2mJeoP9JnWHSRwvLFy4EOvXr0dmZibWr1+PhQsXAgC2bduGO+64AwDw/vvv46uvvsLy5cs7Le02Z84cjBs3DuPGjUNlZSUefvhh12tXF3FlKiJv+TM2ich7jE0iffJXbLbUOdBYoaxMZTADCSN4TEvUG+w3qTusiaMzq288gWNblcJTV7w+ACOnRLse4zxFzel5bhtjUwNt8/1ZE0dzeo1NxqXG2G9qjrEZpsq3tWDV9Uoh1v5jInD9x0PcHmdsao6xSZ0wLnWhR7HJkTg6o16ZitOpiIKIXg+HiIiIAqx6n2oqVSaPZ4mIfIlJHB1pttjRVOUAAJiiBOKGGTVuERERERFR71Tusbp+7j+GSRwiIl9iEkdHLKqVqRLTTDAYeWmfSM9Y15iIiKizqj3tI3GYxCEi8i0mcXREvTJVYjo7PCIiIiIKLg67RJVqOtWAMWYNW0NEFHqYxNER93o4rOJPFEwEi+IQERGh7ogNtlPKUNWYgQbEDGR5ACIiX2ISR0csB9unUyVxJA4RERERBZlKt6lUHIVDRORrTOLoSI16JA4r+RMRERFRkKn8sb2o8QDWwyEi8jkmcXSitdGB+jI7AEAYgYSRnE5FpHssbExEROTGrajxWI7EISLyNSZxdKLmUPtUqoSRJhjNrK9BRERERMFFncThSBwiIt9jEkcn1CtTJWWwwyMKOsy7EhFRmDtVaUfjcWVkuSlKIGEUR5YTEfkakzg6wZWpiIiIiCiYqUfhJGdFwGDkFQ4iIl9jEkcnLAdUK1NxJA5RUJCsiUNERORStbe9qHF/TqUiIvILJnF0wm0kDpcXJyIiIqIgU/kj6+EQEfkbkzg6YLdK1B5pH4mTmM7pVEREREQUXE5+r1pePJsrUxER+QOTODpQe8QGqdSAQ79hRkTE8GshCjqc9k9ERGGspc7hWm1VGIEBYzkSh4jIH5gt0AFLUfvQ00TWwyEiIiKiIFO5u30UTvLoCJiieJpBROQP/OuqA27Li3MqFREREREFmRO72pM4g8ZzKhURkb8wiaMDXJmKiIiIiIIZkzhERIHBJI6XqqurMX36dGRmZmL69OmwWCxdbmc0GpGTk4OcnBzk5+e77i8uLsb555+PzMxM7Pi8yHU/V6Yi8p4v4/KGG26A1Wrt8vlE1DuMTSJ98mVs7v68zHX/QCZxiPqE/SZ1h0kcLy1evBhTp05FUVERpk6disWLF3e5XXR0NAoLC1FYWIg1a9a47n/wwQdx3333Yd/e/YhtHuS6PymD06mIvOWruCwqKkJSUhKWLVvW430LFjYm8kjL2CQiz3wVm7u27EW0NRkAYDQrNXGIyHvsN6k7TOJ4afXq1Zg7dy4AYO7cuVi1alWPnyulxMaNGzFr1iw0lNlhcCgdXXR/A6KSjH5pL1E48FVcevN8IvKMsUmkT76KzZM/tF/l7z/WDGMEr2wQ9QX7TeoOkzheOn78OIYOHQoAGDp0KE6cONHlds3NzcjNzcXEiRNdwVNVVYXExESYTCZYDqiKGndRD2fp0qXIzc1Fbm4ubLb/3979B0dd53ccf33zmyQkARVMBCGQiITwQy4IendFI5w6w0BVTrHUQwYbtXPT0+tVafWmXDueOMPUuZlWZ6h3bexUHMQ7aO+u2nBwnOTu5IcoWMEfQQiB5UeSDfkB+bGbT//YsLsxybL5sd/vd7PPx0yG7Hc/u/vdMK98su/v54evz/0AQkYql5I0adIknT59ut/HS4FsvrnlzRF+B8DoZHc26TeB6IxUNqNZD4dsAtGzq98kl/GJuTsRLFmyRGfPnu1z/IUXXoj6OWpra1VQUKDjx4+rvLxcs2fPVk5OTvD+8J2p8vrZmaqiokIVFRWSpKysrMGcPjAq2ZHLK6wIc6QqKiq01+PVkcrWqF8XGM3clE36TSDEjmyeO3T1Ig7ZBHpzQ79JLuMTRZwIdu7cOeB9EydOlMfjUX5+vjwejyZMmNBvu4KCAknStGnTdMcdd+jQoUN64IEH1NTUJJ/P12tnqvHFzB8GrsaOXKakpKiuri7YDsDVkU3AnWKdza7OLp39oCPY9vr5LGoMRIN+E0PFdKohWr58uSorKyVJlZWVWrFiRZ82Xq9XHR2BTq2+vl7V1dUqKSmRZVm68847tW3btt7TqdiZChiWkcplpMcPiOn/wIAczSaAAY1ENre++mt1NhtJkhnToZwpXCMGhot+ExEZYwbzhR719fWmvLzcFBUVmfLyctPQ0GCMMWb//v1m3bp1xhhjqqurTWlpqZkzZ44pLS01r732WvDxNTU1ZkHZAvNPhZ+YVwprzSuFtabF0xXxNTMzM2P3hhCNwebFzi+YEcrlggVm+vTpZuXKlaa9vT3i6723odG8Ulhr0qwx5qN/a47dG8PVOJ0/cnkVdmfzCvpNxzmdQbJ5FSORzdWz/y74t+wvHzsb1euSTcc5nUGyeRVO9Jvk0hWiyopljBlUzSdWxaRE1Hber9cXnZEkpWZbWvfRDRHn+WdlZamtrc2u00Nfbh5rQTYd8N4Grz5+vVVPnZih/T87pzmPjnX6lBKVW7NJLh1Gv+k4spkAfvODBn3280uSpNv+Nlfz/qLvehxfRTYdRzbRB7l0haiyyXQqB311KlWkAg4AAADgNmcPhhY1vv5r6Q6eCQAkBoo4Dmrqtb0484eBeEYNFgCQaC5d8Kv5ZGCTjuQ06bpZLGoMALFGEcdBjWE7U40rYlFjAAAAxI+zB0O7Ul03J03J6VzRAIBYo4jjIG9N+EgcijgAAACIH54DoSJOPlOpAMAWFHEc1Gs61XSmUwFxh+X3AAAJ7PQfwoo4t1LEAQA7UMRxSMfFbl260C0pMId47GSKOAAAAIgPlxv8ajgauCCZlCLlL6CIAwB2oIjjkPCdqfKmpSopmTnEQFwjwgCABHLm/dAonAlz05SWzccKALADv20d4mVnKgAAAMSpuur24Pc33J7h4JkAQGKhiOMQb01oZ6q86SxqDMQjw5o4AIAEFb4ezg23MZUKAOxCEcch4SNxxhdTxAEAAEB8aD3j08UTgQuSyemWrr+FIg4A2IUijkN6rYnDzlQAAACIE7XvhaZS5ZelKTmdheEAwC4UcRzQdblbLaf9kiQrScqbykgcIN7x5ysAIFHU7g4VcW5cPMbBMwGAxEMRxwFNx31Sz1oaOTemcPUCAAAAccHXYXRqb6iIM6WcRY0BwE4UcRzAzlTAKMHKxgCABHPm/Xb5LgX6v9ypKcqbxohyALATRRwHNIXtTDWuiI4PAAAA8eHkrrBROHcyCgcA7EYRxwGNn4ePxKGIAwAAAPczxujkrsvB21PKWQ8HAOxGEccBTTVhRRx2pgJGB5a2AgCMcvWfdKmlLrA5R2q2pfwFbC0OAHajiGMzf5fRxROh6VR50xmJAwAAAPer+eWl4PdT7xqj5DSuYACA3SjiDEFjY6OWLl2q4uJiLV26VF6vt0+b3bt3a968ecGvjIwMbd++Xc21PnX31HBarAu69Rvz9eGHH9r8DoDRaTjZlKRHH31UhYWFwfuulk3WNQaiY3c2AURnUNmcO097Nn8ePF60LJNsAjFCv4lILDO4TyF8ZJH0zDPPaPz48Vq/fr02btwor9erl156acD2jY2NKioqUl1dnc7+Tnr3LxskSZO/maFllddF/bpZWVlqa2sb9vljyNx8uYlsanjZzMwM/DG6bNkyrVy5MqrX2/N8oz55o01PnZihg5XnVPrI2JF6Kxgct2aTXPawO5tX0G86jmy63GCyef5wp97+03OSpNSxltbuu0HrHl9LNuMT2XQ5J/pNcukKUWWTkThDsGPHDq1Zs0aStGbNmmDFcyDbtm3Tvffeq8zMTHlrwqdSsR4OMJKGk00AsUM2AXcaTDa/CJtKNe1bY5Sc7tY6ABD/6DcRCUWcITh37pzy8/MlSfn5+Tp//nzE9m+++aYefvhhSZL3i9Cixv+85UU9/fTT6ujoiN3JAglkONm84rnnntOcOXMGn02LP2aBgTiaTQADijab/i6jz34RukJftCz0QZFsAiOPfhORUMQZwJIlS1RaWtrna8eOHYN6Ho/HoyNHjujuu++WJHnDdqZ66bUfqrGxMeLQuM2bN6usrExlZWXy+XwDtgMSRayyKUkvvviijh07pv3790eVzbfffnvI7wMYbdyUTfpNIGQksnly12VdbuiWJGVOTNKkb2RIIpvAcLih3ySX8Yn5PAPYuXPngPdNnDhRHo9H+fn58ng8mjBhwoBtt27dqvvuu0+pqaky3UZNX4TCMWFmptauXatNmzYN+PiKigpVVFRICsxTBBJdLLJ5xZUrHunp6VFlc8/JRn2yhbnDgOSubNJvAiEjkc1jW0N93c0rs5WUHBh9SjaBoXNDv0ku4xMjcYZg+fLlqqyslCRVVlZqxYoVA7bdsmVLcGhbyxm/fO2B9boyxicpY1yStm/frtLS0tifNJAAhprNKzwejyTJGEM2gRFENgF3iiabrR6fave0B2/P/Hbogx7ZBGKDfhORUMQZgvXr16uqqkrFxcWqqqrS+vXrJUkHDhzQY489Fmx34sQJnTp1SosXL5bUez2c402HNXv2bNXX1+v555+39w0Ao9RQs3nF6tWrNXv27CFlkyVxgIE5mU0AA4smmx+/3ioTmEmlgtvSlXNjaCA/2QRig34TkbDFuI0+eq1Fv/9xkySp5OEsLX5h/KAez7ZvjnPzx3Sy6YA9zwWmUz11YoY++I/zmrU62+lTSlRuzSa5dBj9puPIZpzrauvW618/o87mwI/sns3XqnDJmGE/L9l0HNlEH+TSFdhi3G3CFzUeV5QaoSUAAADgrE/fbgsWcHKnpGhqeYbDZwQAoIhjo/DpVBRxAAAA4Fb+DqNDm1uCt+eszZaV5NYBHACQOCji2MQYI2/YzlTjprMxGAAAANzp6NZWtZ7xS5IyrknSjJXsXAMAbkARxyaX67vVcTGwKlxqlqWs/GSHzwjAiOLiJABglPC1d+vgvzQHb89/IkepmXxsAAA34LexTXqthzM9VRZb2QAAAMCFPtzcokvnAxcfsyYma9ZqRuEAgFtQxLGJ9/NQESeviKlUwGgwuM39AABwv+ZTPn3wamgtnLK/ylFKBh8ZAMAt+I1sE29N+Ho4LGoMAAAAdzHGaO8Gr/wdgasU15am6uYHGYUDAG5CEccm7EwFAAAANzu6tU0nd7cHb3/zR+OUlMwSAADgJhRxbNIUPhKH6VTAqMMyVwCAeNb0ZZeq/7EpeLv0O9m6/pZ0B88IANAfijg26GjuVtu5wBaNSWlSzmSKOAAAAHCHjuZuvfN4vXyXAtOo8qanaNGzuQ6fFQCgPxRxbBC+M1Xe1FQlpXDJHhgVWNgYABDn/J1GO7/XIO8XgVHjyemWlrx8jVLH8DEBANyI3842aOq1Hg6jcAAAAOA8f5dR1fcaVLsntA7OHRvH6brSNAfPCgAQCRUFG4TvTJXHzlQAAABwWNelbu18ulEnqi4Hj33tuzm6aQW7UQGAm1HEsUH4zlTjiyniAKMSsyQBAHGi5bRP7z5Zrwsfh/5GnbsuWwueznHwrAAA0aCIY4Ne24tP50cOAAAA+xlj9Nn2S9r79151toYWdptXMVaLns2VxVaLAOB6VBRizNfereZTgZ2prCQpdxojcYDRwrCwMQAgTlw40qnqF5rk2dcRPGYlS3/yD+NU8nC2g2cGABgMijgx1vSlL7iDzdjJKUpJ5woHAAAAYs90G9VVd+ijn7XoVNjixZKUOyVFd708XhPnpTt0dgCAoaCIE2NXtmuU2JkKAAAAseXvMrpwpFPH372sml9dUusZf6/7k1KkWX+erYV/navULDaqBYB4Q1Uhxnqvh8NUKmDUYpAdAMBmptuopc6vhmOdqj/apfMfdspzoENdbf3M97Wkwm+N0cK/ydU4pvcDQNyi/D5Eb731lmbNmqWkpCQdOHBgwHZH954Kfv/7T/83+P2XX36phQsXqri4WA899JA6Oztjer5AIog2l++8845mzJihoqIibdy4MXicXAKxQTaByIwx8ncZdbV1q93rV6vHp8bPu+Q50KETv7msT3/RpsP/3qJ9L1/U7mca9d+PnNcbd3n0r7NO6z/v8OidJxp04CfNqt3T3qeAk56bpNLvZOvPduXrnlev7VXAIZuAO5FNRGKZwa3MyTKePY4ePaqkpCQ9/vjj2rRpk8rKyvq08fv9+nFxta5VoSTp55nf10+2/kglJSV68MEHdf/992vVqlV64oknNHfuXD355JMRXzMrK0ttbW0xeT+IipvHWvSbzVaPT7/7odfuc3FMa2urJEuHDx9WSUmJ8vJy+7Qxxmj37t1atGiRMjIy9N57ezV//nyNHZutgwcPKj8/XwUFBTp8+Ihyc3M0ZcqUAV+v4WiXWj1+PXVihg5tOa+Sh1gY0iFuzSZ9Zo9o+8ybbrpJVVVVmjRpkhYsWKAtW7YMuc+U6DddIO6yWVfdro9+2hJYuN4ERroE/u15UHdgUfvg8WC7UPve7XqOm7DnMVJ3l5G/w8jfqZ5/TaDtCMkuSNak2zM07d4xmvT1DCWn9f9fQTYTVtxlM9E4kU1y6QpRZZPpVEM0c+bMq7Z5/w/7NF6Tg7fLv71QO3bs0MyZM7Vr1y698cYbkqQ1a9Zow4YNUXV6wGB0XTI6uav96g1HjcCvtBt0iy5+IF1U/+99um7XhT9KUqem6lY17pca1a5rNUtdF6WTx9qVq2KpXjpZE93Pj11ZgYFF02fu27dPRUVFmjZtmiRp1apV9JmwXds5v2p/G1/9Zsb4JF1zc2rPV5oKbk3X2MnJUW0XTjYBdyKbiGSwI3HwFZZl/VbSD4wxfca5WZa1UtI9xpjHem4/ImmhpA2S/miMKeo5PlnS/xhjSvt5jgpJFT03ZxhjxsbifQCjSaxz2XM/2QQGiWwC7kQ2AXfisyb6w0icCCzL2inp+n7ues4YsyOap+jnmIlwvO9BYzZL2hzFawEJwQ25lMgm8FVkE3Ansgm4kxuySS7jE0WcCIwxS4b5FHVS2HwqaZKkM5LqJeVZlpVijPGFHQdwFeQScCeyCbgT2QTciWxiqNidKrb2Syq2LKvQsqw0Sask/ZcJzGHbLWllT7s1kqKptgIYPnIJuBPZBNyJbALuRDYTFEWcIbIs6z7Lsuok3SbpV5ZlvdtzvMCyrF9LUk/l87uS3pV0VNJWY8z/9TzFs5K+b1nWF5KukfRTu98DMNqQS8CdyCbgTmQTcCeyiUhY2BgAAAAAACAOMBIHAAAAAAAgDlDEAQAAAAAAiAMUcQAAAAAAAOIARRwAAAAAAIA4QBEHAAAAAAAgDlDEAQAAAAAAiAMUcQAAAAAAAOLA/wMjiCOLhDsk6wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1440x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def identity(x):\n",
    "    i = x\n",
    "    return i\n",
    "\n",
    "sign = np.sign\n",
    "\n",
    "def  sigmoid(x):\n",
    "    s = 1/(1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def tanh(x):\n",
    "    t = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "    return t\n",
    "\n",
    "def relu(x):\n",
    "    r = np.maximum(x, 0)\n",
    "    return r\n",
    "\n",
    "x = np.arange(-5, 5, 0.01)\n",
    "\n",
    "\n",
    "activation_funcs = [identity, sign, sigmoid, tanh, relu]\n",
    "function_names = [\n",
    "    'Identity Function', \n",
    "    'Sign Function', \n",
    "    'Sigmoid Function', \n",
    "    'Tanh Function', \n",
    "    'ReLU Function'\n",
    "]\n",
    "\n",
    "fig, ax =  plt.subplots(1, 5, figsize = (20, 4))\n",
    "\n",
    "for i in range(0, len(activation_funcs)):\n",
    "    ax[i].spines['left'].set_position('center')\n",
    "    ax[i].spines['bottom'].set_position('center')\n",
    "    ax[i].spines['right'].set_position('center')\n",
    "    ax[i].spines['top'].set_position('center')\n",
    "    ax[i].xaxis.set_ticks_position('bottom')\n",
    "    ax[i].yaxis.set_ticks_position('left')\n",
    "    ax[i].set_ylim([-1, 1])\n",
    "    \n",
    "    ax[i].plot(\n",
    "        x, \n",
    "        activation_funcs[i](x), \n",
    "        color='#9621E2', \n",
    "        linewidth=3\n",
    "    )\n",
    "    ax[i].set_title(function_names[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1.3 Choice and Number of Output Nodes**\n",
    "* Tie to the activation functions that depend on the application\n",
    "    * e.g., *k*-way classification (intended) $\\Longrightarrow$ _k_ outputs with softmax activation function\n",
    "    * The activation function for te *i*th output is defined as: $$\\Phi(\\bar{v})_i = \\frac{exp(v_i)}{\\sum_{j=1}^{k}exp(v_j)} $$\n",
    "    * An example of the softmax function with three outputs $\\Longrightarrow$ coverting the three outputs into probablilites with the softmax function       \n",
    "\n",
    "#### **1.1.4 Choice of Loss Function**\n",
    "* Hinge loss: $L = max\\left\\{0, 1-y\\cdot\\hat{y}\\right\\}$, where $y\\in{\\left\\{-1, +1\\right\\}}$ $\\Longrightarrow$ SVM\n",
    "\n",
    "##### **1.1.4.1 Binary targets (logistic regression)**\n",
    "* $L = log(1+exp(-y\\cdot{\\hat{y}))}$: assuming the observed value $\\in{\\left\\{-1, +1\\right\\}}$, and prediction $\\hat{y}$ is an arbitrary numerical value on suing the identity activation function.\n",
    "\n",
    "##### **1.1.4.2 Categorical targets (using the softmax activation function)**\n",
    "* $L = -log(\\hat{y}_r)$: implementing multinomial logistic regression  $\\Longrightarrow$ <font color=red>*cross-entropy loss*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>**1.2 Multilayer Neural Networks**</font>                                                                                   \n",
    "Multiple computational layers: the ***hidden layers*** between input an outputs\n",
    "#### 1.2.1 Example: Feed-forward network\n",
    "* sequentially feed the successive layers in the forward direction\n",
    "* assume all nodes in one layer are connected to those of the next layer\n",
    "* The weights of the connections b/w the input layer and the first hidden layer are contained in a _matrix_ $W_1$ with size $d\\times{p_1}$, where the weights b/w the *r*th hidden layer and the (*r*+1)th hidden layer are denoted by the $p_r\\times{p_{r+1}}$ matrix denoted by $W_r$.\n",
    "    * $\\overline{h}_1 = \\Phi(W_1^T\\overline{x})$ $\\Longrightarrow$ <font color=brown>Input to Hidden Layer</font>\n",
    "    * $\\overline{h}_{p+1} = \\Phi(W_{p+1}^T\\overline{h}_p)$     $\\forall{p}\\in{\\left\\{1,..., k-1\\right\\}}$ $\\Longrightarrow$ <font color=brown>Hidden to Hidden Layer</font>\n",
    "    * $\\overline{o} = \\Phi(W_{k+1}^T\\overline{h}_k)$ $\\Longrightarrow$ <font color=brown>Hidden to Output Layer</font>\n",
    "\n",
    "#### 1.2.2 Example: Convolutional network\n",
    "* weight pruning and sharing: e.g., image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>**1.3 Training Neural Network with Backpropagation**</font>                                        \n",
    "* **Backrpogation algorithm**: <font color=blue>computing the error gradients in terms of summations of local-gradient products over the various paths froma node the ouput</font>\n",
    "    * A direct application of dynamic programming\n",
    "    * Two phases: _forward_ vs *backword*\n",
    "    \n",
    "#### 1.3.1 Forward phase: the inputs for a training instance are fed into the neural network\n",
    "* A forward cascade of computation across the layers with current set of weights\n",
    "* The derivative of the loss fuction with respect to the output is computed\n",
    "* The derivative will be computed with respect to the weights in all layers in the backward phase                            \n",
    "\n",
    "#### 1.3.1 Backward phase: compute the graident of the loss function with respect to the different weights by using the chain rule of differential calculus\n",
    "* Gradient for updating the weights\n",
    "* Starting from the ouput node\n",
    "\n",
    "<font size=4 color=red>**1.4 Practical Issues in Neural Network Training**</font>     \n",
    "#### 1.4.1 Overfitting\n",
    "* Remedies\n",
    "    * Regularization\n",
    "    * Architecture and Parameter Sharing\n",
    "    * Early Stopping\n",
    "    * Breadth vs Depth\n",
    "    * Ensemblre\n",
    "\n",
    "#### 1.4.2 Gradient Vanishing and Explodeing\n",
    "\n",
    "#### 1.4.3 Hardly Convergence\n",
    "\n",
    "#### 1.4.4 Local and Spurious Optima\n",
    "\n",
    "#### 1.4.5 Computational Challenges\n",
    "\n",
    "<font size=4 color=red>**1.5 The Power of Function Composition**</font>                                      \n",
    "**'Much of the power of deep learning arises from the fact that the <font color=red>_repeated composition of certain types of functions increases the representation power of the network, and therefore reduces the parameter space requred for learning_</font>'**           \n",
    "\n",
    "* <font color=green>**Theorem**</font>: _A multi-layer network that uses only the identity activation function in all its layers reduces to a single-layer network performing linear regression_.\n",
    "* <font color=brown>**Lemma**</font>: _Consider a multilayer network in which all hidden layers use identity activation and the single output node uses the perceptron criterion as the loss function and sign activation for prediction. This neural network reduces to the single-layer perceptron._\n",
    "\n",
    "#### 1.5.1 Nonlinear Activation\n",
    "* <font size=3 color=red>Activation functions enable nonlinear mappings of the data, so that the embedded points can become linearly separable.</font>\n",
    "* e.g., A=(-1,1), B=(0,1), C=(1,1)\n",
    "    * Not linearly separable $\\Longrightarrow$ Linear activation NOT working $\\Longrightarrow$ Nonlinear activation\n",
    "        * Hidden units applied with ReLU activation function\n",
    "        $$h_1 = max{\\left\\{x_1, 0\\right\\}}$$\n",
    "        $$h_2 = max{\\left\\{-x_1, 0\\right\\}}$$\n",
    "    * A=(0,1), B=(0,0), C=(1,0) $\\Longrightarrow$ linearly separable\n",
    "* XOR function separating points {(0,0), (1,1)} and {(1,0),(0,1)} into two different classes.\n",
    "\n",
    "#### 1.5.2 Reducing Parameter Requriment with Depth (breadth vs depth)\n",
    "\n",
    "<font size=4 color=red>**1.6 Common Neural Architectures**</font>                                              \n",
    "#### 1.6.1 Shallow Models (1 or 2 layers)\n",
    "* linear regression, classification, support vector machine, logistic regression, singular value decomposition, matrix factorization, and etc\n",
    "* Similating and learning neural networks\n",
    "* Deep learning: stacking the simpler models in a creative way\n",
    "\n",
    "#### 1.6.2 Radial Basis Function Network (RBF)\n",
    "* Not deep, typically having two only two layers\n",
    "* 1st layer: unsupervised training; 2nd layer: supervised training\n",
    "* Gain insights from expanding the size of the feature space rather than depth\n",
    "* 2nd layer training: supervised nearest neighborhood classification\n",
    "\n",
    "#### 1.6.3 Restricted Boltzmann Machines (RBMs)\n",
    "* Using the notion of energy minimization to modeling data in an unsupervised way\n",
    "* Requiring Monte Carlo sampling in order to train\n",
    "* Algorithm: _constrastive divergence algorithm_\n",
    "\n",
    "#### 1.6.4 Recurrent Neural Networks\n",
    "* Designedd for sequential data, e.g., text sentence, time-series, biological sequences, and etc\n",
    "* Backgpropagation algorithm: Backpropagation through time (BPTT)\n",
    "\n",
    "#### 1.6.5 Convolutional Neural Network (a sparse architecture)\n",
    "* **Image classification and object detection**\n",
    "* Hubel and Wiesel's understanding of how cat's visual cortex works: specific portion of the visual field excites particular neurons\n",
    "* <font color=red>Each layer</font>: 3-dimensional (corresponding to RGB in input layer or hidden features maps that encode various types of the shape in the image)\n",
    "* Two types of layers: ***convolution*** and ***subsampling*** layers\n",
    "    * Convolution operation: a filter to map the activations from one layer to the next\n",
    "    \n",
    "#### 1.6.6 Hierarchical Feature Engineering and Pretrain Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Basics of Feedforward Neural Network in Keras**\n",
    "* Remember that each _unit_ (node or neuron) in the hidden layers:\n",
    "    * 1. Receives a number of inputs\n",
    "    * 2. Weights each input by a parameter value\n",
    "    * 3. Sums together all weighted inputs along with some bias (typically 1)\n",
    "    * 4. Most often then applies some funciton (called an _activation function_)\n",
    "    * 5. Sends the output on to units in the next layer\n",
    "\n",
    "#### <font color=blue>General requirments for constructing a neural network</font>\n",
    "* <font size= 3 color=red>**First**</font>: Define the number of the units to include in each layer and the activation funciton in the hidden and output layers.\n",
    "    * The more units, the more to learn complex patterns\n",
    "        * __Caution__: overfit the training data\n",
    "        * A popular activation function: <font color=red>_rectified linear unit (Re LU)_</font>\n",
    "        $f(z) = max(0,z)$, where $z$ i sthe sum of the weighted inputs and bias.\n",
    "        * other activation fuctions: e.g., sigmoid, softmax, etc                                        \n",
    "\n",
    "* <font size= 3 color=red>**Second**</font>: Define the number of the hidden layers\n",
    "    * Complex relationship vs computational cost\n",
    "\n",
    "* <font size= 3 color=red>**Third**</font>: Define the structure of the activation function of the output layer and loss function\n",
    "    * <font color=blue>Binary classification</font>: Binary cross-entropy\n",
    "    * <font color=orange>Multiclass classification</font>: Categorical cross-entropy\n",
    "    * <font color=brown>Regression</font>: Mean square error ($MSE = \\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}(\\hat{y_i}-y_i)^2$)\n",
    "\n",
    "* <font size= 3 color=red>**Fourth**</font>: Define the optimizer\n",
    "    * e.g., stochastic gradient descent (SGD),stochastic gradient descent with momentum, root mean square propagation, adaptive moment estimation\n",
    "\n",
    "* <font size= 3 color=red>**Fifth**</font>: Evaluate performance with one or multiple metrics\n",
    "\n",
    "#### <font color=blue>Keras: Two ways to create neural works</font>\n",
    "* Sequential model: stacking layers togeter\n",
    "* Functional API: for researchers rather than practitioners\n",
    "\n",
    "#### <font color=blue>Some basic parameters of the Keras</font>\n",
    "* '<font color=red>dense</font>': fully connected, all units in the previous layer are connected to all the neurals in the next layer\n",
    "* '<font color=red>units =</font>': the number of the units with activation function\n",
    "* '<font color=red>activation = 'relu'</font>': ReLU activation function\n",
    "* '<font color=red>input_shape = (10, )</font>': the first layer of any network has to include this parameter, e.g., 10 feature values\n",
    "* '<font color=red>RMSProp</font>': a optimization algorithm\n",
    "* '<font color=red>binary_crossentropy</font>': a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Building a Neural Network Model with Keras\n",
    "* Binary classifier\n",
    "* Multiclass classifier\n",
    "* Regression\n",
    "\n",
    "<font color=brown>**2.1.1 Binary classifier**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "np.random.seed(9)\n",
    "\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words = number_of_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words =  number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode = 'binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode = 'binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units = 16, activation = 'relu', input_shape = (number_of_features, )))\n",
    "network.add(layers.Dense(units = 16, activation = 'relu'))\n",
    "network.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "network.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = 'rmsprop',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "history = network.fit(features_train,\n",
    "                      target_train, \n",
    "                      epochs = 10,\n",
    "                      verbose = 1,\n",
    "                      batch_size = 100,\n",
    "                      validation_data = (features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**2.1.2 Multiclass classifier**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 5000\n",
    "\n",
    "data = reuters.load_data(num_words = number_of_features)\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "\n",
    "tokenizer = Tokenizer(num_words = number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode = 'binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode = 'binary')\n",
    "\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units = 100, activation = 'relu', input_shape = (number_of_features, )))\n",
    "network.add(layers.Dense(units = 100, activation = 'relu'))\n",
    "network.add(layers.Dense(units = 46, activation = 'softmax'))\n",
    "\n",
    "network.compile(loss = 'categorical_crossentropy',\n",
    "                optimizer = 'rmsprop',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "history = network.fit(features_train, \n",
    "                      target_train,\n",
    "                      epochs = 10,\n",
    "                      verbose = 1,\n",
    "                      batch_size = 100,\n",
    "                      validation_data = (features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**2.1.3 Regression**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.9 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(19)\n",
    "\n",
    "features, target = make_regression(\n",
    "    n_samples=10000,\n",
    "    n_features=3, \n",
    "    n_informative=3, \n",
    "    n_targets=1,\n",
    "    noise=0.0,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size = 0.33, random_state = 0\n",
    ")\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(\n",
    "    layers.Dense(\n",
    "        units=32, \n",
    "        activation='relu',\n",
    "        input_shape=(features_train.shape[1], )\n",
    "    )\n",
    ")\n",
    "network.add(\n",
    "    layers.Dense(\n",
    "        units = 32,\n",
    "        activation='relu')\n",
    ")\n",
    "network.add(layers.Dense(units = 1))\n",
    "network.compile(\n",
    "    loss = 'mse',\n",
    "    optimizer = 'RMSprop',\n",
    "    metrics = ['mse']\n",
    ")\n",
    "\n",
    "history =  network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=10,\n",
    "    verbose=1, \n",
    "    batch_size=100, \n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "training_accuracy = history.history['mean_squared_error']\n",
    "test_accuracy = history.history['val_mean_squared_error']\n",
    "\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training MSE', 'Test MSE'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Making Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 10000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words = number_of_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words = number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode = 'binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode = 'binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units = 16,\n",
    "                         activation = 'relu',\n",
    "                         input_shape = (number_of_features, )))\n",
    "network.add(layers.Dense(units = 16,\n",
    "                         activation = 'relu'))\n",
    "network.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "network.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = 'rmsprop',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "history = network.fit(features_train,\n",
    "                      target_train,\n",
    "                      epochs = 5,\n",
    "                      verbose = 1,\n",
    "                      batch_size = 100,\n",
    "                      validation_data = (features_test, target_test))\n",
    "\n",
    "predict_target = network.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optimizing the model (Reducing Overfitting)\n",
    "* Weight regularization\n",
    "* Early Stopping\n",
    "* Dropout\n",
    "\n",
    "#### 2.3.1 Weight regularization\n",
    "A penalty is added to the loss function, e.g., L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers, regularizers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words = number_of_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words = number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode = 'binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode = 'binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units = 16,\n",
    "                         activation = 'relu',\n",
    "                         kernel_regularizer = regularizers.l2(0.01),\n",
    "                         input_shape = (number_of_features,)))\n",
    "network.add(layers.Dense(units = 16,\n",
    "                         kernel_regularizer = regularizers.l2(0.01),\n",
    "                         activation = 'relu'))\n",
    "network.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "network.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = 'rmsprop',\n",
    "                metrics = ['accuracy'])\n",
    "history = network.fit(features_train, \n",
    "                      target_train,\n",
    "                      epochs = 20,\n",
    "                      verbose = 0,\n",
    "                      batch_size = 100,\n",
    "                      validation_data = (features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Early Stopping\n",
    "* Monitoring the training process and stop training when the test error starts to increase.\n",
    "* Saving the model after every checkpoint: useful in case a multiday training sesssion is interrupted for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words = number_of_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words = number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode = 'binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode = 'binary')\n",
    "\n",
    "network =  models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units = 16,\n",
    "                         activation = 'relu',\n",
    "                         input_shape = (number_of_features, )))\n",
    "network.add(layers.Dense(units = 16, activation = 'relu'))\n",
    "network.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "network.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = 'rmsprop',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "callbacks = [EarlyStopping(monitor = 'val_loss', patience =2),\n",
    "             ModelCheckpoint(filepath = 'best_model.h5',  \n",
    "                             monitor = 'val_loss',\n",
    "                             save_best_only =  True)]\n",
    "history = network.fit(features_train,\n",
    "                      target_train,\n",
    "                      epochs = 10,\n",
    "                      callbacks = callbacks,\n",
    "                      verbose = 0,\n",
    "                      batch_size =100,\n",
    "                      validation_data = (features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Dropout\n",
    "* A proportion of the units in one or more layers is multiple by zero (dropped): <font color=red>each batch is confronted by a slightly different version of the network's architecture</font>\n",
    "* Constantly and randomly dropping out units in each batch: forcing the uits to learn parameter values able to perform under a wide variety of network architectures $\\Longrightarrow$ <font color=blue>preventing the network from simply 'memorizing' the training data.</font>\n",
    "* Dropout can be added to both hidden and input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 10000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words = number_of_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words = number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode = 'binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode = 'binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dropout(0.2, input_shape = (number_of_features, )))\n",
    "network.add(layers.Dense(units = 16, activation = 'relu'))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units = 16, activation = 'relu'))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "network.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = 'rmsprop',\n",
    "                metrics = ['accuracy'])\n",
    "history = network.fit(features_train,\n",
    "                      target_train,\n",
    "                      epochs = 10,\n",
    "                      verbose = 0,\n",
    "                      validation_data = (features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Saving Model Training Progress\n",
    "* '<font color=red>save_best_only = True</font>' and '<font color=blue>monitor = 'val_loss'</font>': no override a file if the model has a worse test loss than the previous model\n",
    "* '<font color=brown>filepath = model_{epoch:02d}*_*{val_loss:.2f}.hdf5</font>' will save every epoch's model with epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Add the function below to save the best model\n",
    "checkpoint = [ModelCheckpoint(filepath = 'models.hdf5')]\n",
    "\n",
    "history = network.fit(features_train,\n",
    "                      target_train,\n",
    "                      epochs = 10,\n",
    "                      callbacks = checkpoint,\n",
    "                      verbose = 0,\n",
    "                      batch_size = 100,\n",
    "                      validation_data = (features_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 k-Fold Cross-validation Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 100\n",
    "\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "                                       n_features = number_of_features,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,\n",
    "                                       n_classes = 2,\n",
    "                                       weights = [0.5, 0.5],\n",
    "                                       random_state = 0)\n",
    "\n",
    "def create_neural_network():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units = 16,\n",
    "                             activation = 'relu',\n",
    "                             input_shape = (number_of_features,)))\n",
    "    network.add(layers.Dense(units = 16,\n",
    "                             activation = 'relu'))\n",
    "    network.add(layers.Dense(units = 1,\n",
    "                             activation = 'sigmoid'))\n",
    "    network.compile(loss = 'binary_crossentropy',\n",
    "                    optimizer = 'rmsprop',\n",
    "                    metrics = ['accuracy'])\n",
    "    return network\n",
    "\n",
    "neural_network = KerasClassifier(build_fn = create_neural_network,\n",
    "                                 epochs = 10,\n",
    "                                 batch_size = 100,\n",
    "                                 verbose = 0)\n",
    "\n",
    "cross_val_score(neural_network, features, target, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Tuning Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 100\n",
    "\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "                                       n_features = number_of_features,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,\n",
    "                                       n_classes = 2,\n",
    "                                       weights = [0.5, 0.5],\n",
    "                                       random_state = 0)\n",
    "\n",
    "def create_neural_network(optimizer = 'rmsprop'):\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units = 16, \n",
    "                             activation = 'relu',\n",
    "                             input_shape = (number_of_features,)))\n",
    "    network.add(layers.Dense(units = 16, activation = 'relu'))\n",
    "    network.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "    network.compile(loss = 'binary_crossentropy',\n",
    "                    optimizer = optimizer,\n",
    "                    metrics = ['accuracy'])\n",
    "    return network\n",
    "\n",
    "neural_network = KerasClassifier(build_fn = create_neural_network, verbose = 0)\n",
    "\n",
    "epoches = [5, 10]\n",
    "batches = [5, 10, 100]\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "\n",
    "hyperparameters = dict(optimizer = optimizers, epochs = epoches, batch_size = batches)\n",
    "\n",
    "grid =GridSearchCV(estimator = neural_network, param_grid = hyperparameters)\n",
    "grid_result =  grid.fit(features, target)\n",
    "print(grid_results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Classifying Images\n",
    "#### 2.7.1 Convolutional neural network (ConvNets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "\n",
    "network = Sequential()\n",
    "\n",
    "network.add(Conv2D(filters = 64,\n",
    "                   kernel_size = (5, 5),\n",
    "                   input_shape = (channels, width, height),\n",
    "                   activation = 'relu'))\n",
    "network.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(128, activation = 'relu'))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(number_of_classes, activation = 'softmax'))\n",
    "\n",
    "network.compile(loss = 'categorical_crossentropy',\n",
    "                optimizer = 'rmsprop',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "network.fit(features_train, \n",
    "            target_train,\n",
    "            epochs = 2,\n",
    "            verbose = 0,\n",
    "            batch_size = 1000,\n",
    "            validation_data = (features_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.2 Improving Performace with Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "augmentation = ImageDataGenerator(featurewise_center = True,\n",
    "                                  zoom_range = 0.3,\n",
    "                                  width_shift_range = 0.2,\n",
    "                                  horizontal_flip = True,\n",
    "                                  rotation_range = 90)\n",
    "augment_images = augmentation.flow_from_directory('raw/images',\n",
    "                                                  batch_size = 32,\n",
    "                                                  class_mode ='binary',\n",
    "                                                  save_to_dir = 'processed/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Classifying Text\n",
    "#### 2.8.1 Applying the recurrent neural network\n",
    "* Key feature: information loops back in the network\n",
    "* Providing a type memory for better understanding sequential data\n",
    "* Popular one: long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models, layers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words = number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras.datasets.imdb.load_data(path='imdb.npz', num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3, **kwargs)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'<font color=red><b>pad_sequences</b></font>': padding each observation's data for the same size since the original data have different number of words (inequal length of each data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = sequence.pad_sequences(data_train, maxlen = 400)\n",
    "features_test = sequence.pad_sequences(data_test, maxlen = 400)\n",
    "\n",
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'<font color=blue><b>Embedding</b></font>' layer for natural language processing: representing each word as a vector in a multidimensional space, allowing the distance between two vectors to represent the similariyt between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.add(layers.Embedding(input_dim = number_of_features, output_dim = 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'<font color=brown><b>LSTM</b></font>': a long short-term memory layer with 128 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.6754 - acc: 0.6025 - val_loss: 0.6601 - val_acc: 0.6007\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.6339 - acc: 0.6722 - val_loss: 0.5922 - val_acc: 0.7188\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.4846 - acc: 0.7749 - val_loss: 0.4161 - val_acc: 0.8170\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 111s 4ms/step - loss: 0.3944 - acc: 0.8301 - val_loss: 0.3982 - val_acc: 0.8271\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 110s 4ms/step - loss: 0.3615 - acc: 0.8489 - val_loss: 0.3632 - val_acc: 0.8440\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 105s 4ms/step - loss: 0.3302 - acc: 0.8651 - val_loss: 0.3360 - val_acc: 0.8573\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.3120 - acc: 0.8722 - val_loss: 0.3268 - val_acc: 0.8641\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 105s 4ms/step - loss: 0.3035 - acc: 0.8758 - val_loss: 0.3269 - val_acc: 0.8614\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.3033 - acc: 0.8782 - val_loss: 0.3228 - val_acc: 0.8636\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.2859 - acc: 0.8838 - val_loss: 0.3177 - val_acc: 0.8641\n"
     ]
    }
   ],
   "source": [
    "network.add(layers.LSTM(units = 128))\n",
    "network.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "network.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = 'Adam',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "history = network.fit(features_train,\n",
    "                      target_train,\n",
    "                      epochs = 10,\n",
    "                      verbose = 1,\n",
    "                      batch_size = 1000,\n",
    "                      validation_data = (features_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. TensorFlow Basics**\n",
    "* <font size=3, color=brown>Powerful library by Google for numerical computation for large-scale Machine Learning</font>\n",
    "* <font size=3, color=green>Simply put: Python program defines a graph of computations to perform, and then TensorFlow take that graph and runs it efficiently using optimized C++ code</font>\n",
    "\n",
    "<img src ='fig9_1.png' width=400><img src ='fig9_2.png' width=400>\n",
    "\n",
    "#### **Some Open Source Deep Learning libraries**\n",
    "\n",
    "| Library | API | Paltforms | Started by | Yeaer |\n",
    "|:-------|:----|:--------|:----------|:-----:|\n",
    "|Caffe|Python,C++,Matlab|Linux,macOS,Windows|Y.Jia,UC Berkeley(BVLC)|2013|\n",
    "|Deeplearning4j|Java,Scala,Clojure|Linux,macOS,Windows,Android|A.Gibson,J.Patterson|2014|\n",
    "|H2O|Python,R|Linux,macOS,Windows|H2O.ai|2014|\n",
    "|MXNet|Python,C++,others|Linux,macOS,Windows,iOS,Android|DMLC|2015|\n",
    "|TensorFlow|Pyhton,C++|Linux,macOS,Windows,iOS,Android|Google|2015|\n",
    "|Theano|Python|Linux,macOS,iOS|University of Montreal|2010|\n",
    "|Torch|C++,Lua|Linux,macOS,iOS,Android|R.Collobert,K.Kavukcuoglu,C.Farabet|2002|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Tensorflow returns a reference to the desired tensor rather than the value of the tensor ifself; \n",
    "#### * <font color=green>_.eval()_</font> is function for returning the value of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "[[5 5]\n",
      " [5 5]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.InteractiveSession()\n",
    "\n",
    "a = tf.zeros(2) # fill the tensors with zeros\n",
    "print(a.eval())\n",
    "\n",
    "b = tf.ones((2,3))  # filling the tensors with ones\n",
    "print(b.eval())\n",
    "\n",
    "c = tf.fill((2, 2), value = 5) # filling the tensors with arbitrary values\n",
    "print(c.eval())\n",
    "\n",
    "d = tf.constant(3) # creating a constant tensors\n",
    "print(d.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1510291   0.31776264]\n",
      " [ 0.03440975 -1.634231  ]]\n",
      "[[ 0.20355405 -1.2773987 ]\n",
      " [ 0.9960875  -1.3473536 ]]\n",
      "[[-1.5328822   0.55277014]\n",
      " [ 1.4545512  -0.44041824]]\n"
     ]
    }
   ],
   "source": [
    "# random Normal entries\n",
    "a_rand = tf.random_normal((2, 2), mean = 0, stddev = 1)\n",
    "print(a_rand.eval())\n",
    "# Reduce the numerical insteability of large samples; drops and resamples all values more than two standard deviations from the mean\n",
    "a_rand =  tf.truncated_normal((2, 2), mean =0, stddev = 1)\n",
    "print(a_rand.eval())\n",
    "# random sample from the Uniform distribution over a specific range \n",
    "a_rand = tf.random_uniform((2,2), minval = -2, maxval =2)\n",
    "print(a_rand.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n"
     ]
    }
   ],
   "source": [
    "r = tf.range(1, 5, 1) # tf.range(start, limit, delta)\n",
    "d = tf.diag(r)\n",
    "print(d.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful basic functions\n",
    "tf.matrix_transpose # transpose a matrix\n",
    "tf.eye # identify matrix\n",
    "tf.matmul # matrices multiplication\n",
    "tf.get_default_graph() # Getting the default TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1.1 First Graph and Running it in Session**\n",
    "* <font size=3, color=green>Step 1. Building a computation graph$\\Longrightarrow$ ML model and the computation $\\Longrightarrow$ <font color=red>_construction_ phase</font></font>\n",
    "* <font size=3, color=green>Step 2. Run the computation$\\Longrightarrow$ Model evaluation repeatedly $\\Longrightarrow$ <font color=red>_execution_ phase</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Creat a graph (function)\n",
    "x = tf.Variable(3, name = 'x')\n",
    "y = tf.Variable(4, name = 'y')\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "# tf.Variable() provides a wrapper around the tensors that allows for stateful computations\n",
    "# Great a varaible with tensor inside\n",
    "a = tf.Variable(tf.ones(2,2))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(a.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# tf.Session() stores the context under which a computation is performed.\n",
    "# Regular Session\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "\n",
    "result =  sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have to repeat the session\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the global initializer to every single variable\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # initialize all the variables\n",
    "    result =  f.eval()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# Interactive Session; you need manually close the 'with' block\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1.2 Managing Graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any Node is automatically added to the default graph:\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Manage multiple independant graphs; need to creat Graph abd temporarily making it the default graph inside a 'with' block\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=3, color=brown>To remove (reset) the many duplicate nodes in a default graph, just reset the default graph by running <font color=red>'*tf.reset_default_graph()*'</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1.3 Lifecycle of a Node Value**\n",
    "* <font size=3, color=green>All node values are dropped between the graph runs</font>\n",
    "* <font size=3, color=green>Variable values maintain until the sesssion is closed</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y and z values are 10 and 15 , respectively.\n"
     ]
    }
   ],
   "source": [
    "# An efficient way to evaluating the results\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print('y and z values are', y_val,'and', z_val,', respectively.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Linear Regression with TensorFlow**\n",
    "* <font size=3>The inputs and outputs are multidimensional arrays (1-_n_ array(s)), called <font color=green>*tensors*</font></font>\n",
    "* <font size=3>Python API tensors were simply represented by NumPy ndarrays (e.g., float, integar, string)</font>\n",
    "* <font size=3>The matrix functions (e.g., _transpose()_, _matmul()_, *matrix_inverse()*) do NOT be performed; instead, they create nodes in the graph that will perform them when the graph is run.</font>\n",
    "* <font size=3>Using <font color=green>*Normal Equation*</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = 'y')\n",
    "\n",
    "XT = tf.transpose(X)\n",
    "\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "\n",
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Implementing Gradient Descent**\n",
    "* <font size=3>Using <font color=brown>*Batch Gradient Descent*</font> intead of <font color=green>*Normal Equation*</font></font>\n",
    "* <font size=3>Normalize the input feature vectors: TensorFlow, NumPy, Scikit-Learn's <font color=brown>_StandardScaler_</font></font>\n",
    "\n",
    "#### **3.3.1. Manually Computing**\n",
    "* <font size=3 color=red> Some Important Notes</font>   \n",
    "    * *random_uniform()* $\\Longrightarrow$ Creating a node in a graph that will generate a tensor containing random values\n",
    "    $\\Longrightarrow$ similar to NumPy *rand()*\n",
    "    * *assing()* $\\Longrightarrow$ Creating a node that will assign a new value to a variable: $\\theta^{(next \\ step)} = \\theta - \\eta\\bigtriangledown_{\\theta}MSE(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE 8.116656\n",
      "Epoch 100 MSE 4.9139776\n",
      "Epoch 200 MSE 4.850081\n",
      "Epoch 300 MSE 4.837301\n",
      "Epoch 400 MSE 4.8288536\n",
      "Epoch 500 MSE 4.822629\n",
      "Epoch 600 MSE 4.817987\n",
      "Epoch 700 MSE 4.8145165\n",
      "Epoch 800 MSE 4.8119063\n",
      "Epoch 900 MSE 4.8099337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_func = StandardScaler()\n",
    "scaled_housing_data_plus_bias = std_func.fit(housing_data_plus_bias).transform(housing_data_plus_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name = 'y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name = 'theta')\n",
    "y_pred = tf.matmul(X, theta, name = 'predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = 'mse')\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, 'MSE', mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3.2. Using autodiff**\n",
    "* <font size=3>TensorFlow's <font color=darkorange>_autodiff_</font>: automatically and efficiently compute the gradients</font>\n",
    "    * <font size=3><font color=red>_tf.gradient()_</font> take an op and a list of variables, and it creates a list of ops (one per variable) to compute the gradients of the op with regards to each variable</font>\n",
    "    * <font size=3> <font color=green>_gradients_</font> node will compute the gradient vector of the MSE with regard to theta</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE 9.886628\n",
      "Epoch 100 MSE 5.098451\n",
      "Epoch 200 MSE 4.974345\n",
      "Epoch 300 MSE 4.9305234\n",
      "Epoch 400 MSE 4.898896\n",
      "Epoch 500 MSE 4.8754587\n",
      "Epoch 600 MSE 4.858012\n",
      "Epoch 700 MSE 4.844984\n",
      "Epoch 800 MSE 4.835213\n",
      "Epoch 900 MSE 4.827857\n"
     ]
    }
   ],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, 'MSE', mse.eval())\n",
    "        sess.run(training_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=3 color=orange>**Main solution to compute gradient automatically**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Technique|No of graph traversal to compute all gradient |Accuracy|Supports arbitary code|Comment|\n",
    "|:--------|:--------------------------------------------|:-------|:---------------------|:------|\n",
    "|Numerical differentiation|$n_{inputs}$ +1|Low|Yes|Trival to implement|\n",
    "|Symbolic differentiation|N/A|High|No|Builds a very different graph|\n",
    "|Forward-mode autodiff|$n_{inputs}$|High|Yes|Uses _dual numbers_|\n",
    "|Reserse-mode autodiff|$n_{inputs}$ +1|High|Yes|Implemented by TensorFlow|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=3 color=red>Using an Optimizer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descend optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# Using Momentum optimizer\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 Feeding Data to the Training Algorithm**\n",
    "#### **3.4.1. Mini-batch Gradient Descent**\n",
    "* Creating a TensorFlow node as placehold ( *placeholder()* ) to hold output\n",
    "* No computation at the node\n",
    "* Optionally, shape cane be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape = (None, 3)) # any size, three columns\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict = {A:[[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict = {A:[[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_func = StandardScaler()\n",
    "scaled_housing_data_plus_bias = std_func.fit(housing_data_plus_bias).transform(housing_data_plus_bias)\n",
    "housing_target = housing.target\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n + 1), name = 'X')\n",
    "y = tf.placeholder(tf.float32, shape = (None, 1), name = 'y')\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name = 'theta')\n",
    "y_pred = tf.matmul(X, theta, name = 'predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = 'mse')\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Something wrong with the codes\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    [...] # load the data from disk  \n",
    "    return X_batch, y_batch\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y:y_batch})\n",
    "\n",
    "best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5 Saving and Restoring Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_func = StandardScaler()\n",
    "scaled_housing_data_plus_bias = std_func.fit(housing_data_plus_bias).transform(housing_data_plus_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name = 'y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name = 'theta')\n",
    "y_pred = tf.matmul(X, theta, name = 'predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = 'mse')\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0: # checkpoint every 100 epochs\n",
    "            save_path = saver.save(sess, '/tmp/my_model.ckpt')\n",
    "            \n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, 'tmp/my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x_zeros = np.random.multivariate_normal(mean=np.array((-1, -1)), \n",
    "                                        cov=0.1*np.eye(2), size=((np.int(N/2)),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-115-1d1412d29ae7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-115-1d1412d29ae7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --logdir='/tmp/my_model_final.ckpt'\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
